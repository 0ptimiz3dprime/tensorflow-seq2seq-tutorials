{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced dynamic seq2seq with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we elected to manually feed `decoder_inputs` to better understand what is going on. Here we implement decoder with `tf.nn.raw_rnn` and will construct `decoder_inputs` step by step in the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projections\n",
    "\n",
    "Here we manually setup input and output projections. It is necessary because we're implementing decoder with manual step transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def projection(inputs, projection_size, scope):\n",
    "    input_size = inputs.get_shape()[-1].value \n",
    "    # inputs shape like [time, batch, input_size] or [batch, input_size]\n",
    "\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        W = tf.get_variable(name='W', shape=[input_size, projection_size],\n",
    "                            dtype=tf.float32)\n",
    "\n",
    "        b = tf.get_variable(name='b', shape=[projection_size],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.constant_initializer(0, dtype=tf.float32))\n",
    "\n",
    "    input_shape = tf.unstack(tf.shape(inputs))\n",
    "\n",
    "    if len(input_shape) == 3:\n",
    "        time, batch, _ = input_shape  # dynamic parts of shape\n",
    "        inputs = tf.reshape(inputs, [-1, input_size])\n",
    "\n",
    "    elif len(input_shape) == 2:\n",
    "        batch, _depth = input_shape\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Wierd input shape: {}\".format(inputs))\n",
    "\n",
    "    linear = tf.add(tf.matmul(inputs, W), b)\n",
    "\n",
    "    if len(input_shape) == 3:\n",
    "        linear = tf.reshape(linear, [time, batch, projection_size])\n",
    "\n",
    "    return linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "We are replacing unidirectional `tf.nn.dynamic_rnn` with `tf.nn.bidirectional_dynamic_rnn` as the encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import (LSTMCell, LSTMStateTuple,\n",
    "                                    InputProjectionWrapper,\n",
    "                                    OutputProjectionWrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('EncoderInputProjection') as scope:\n",
    "    encoder_inputs_onehot = tf.one_hot(encoder_inputs, vocab_size)\n",
    "    encoder_inputs_projected = projection(encoder_inputs_onehot, input_embedding_size, scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'EncoderInputProjection/Reshape_1:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs_projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_projected,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ReverseSequence:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to concatenate forward and backward outputs and state. In this case we will not discard outputs, they would be used for attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_outputs = tf.concat_v2((encoder_fw_outputs, encoder_fw_outputs), 2)\n",
    "\n",
    "encoder_final_state_c = tf.concat_v2(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat_v2(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## - encoder override with forward-only rnn\n",
    "# with tf.variable_scope('encoder_override'):\n",
    "#     encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "#         cell=encoder_cell,\n",
    "#         inputs=encoder_inputs_projected,\n",
    "#         dtype=tf.float32, time_major=True,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# time and batch dimensions are dynamic, i.e. they can change in runtime, from batch to batch\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))\n",
    "\n",
    "# how far to run the decoder is our decision\n",
    "decoder_lengths = encoder_inputs_length + 3\n",
    "# +2 additional steps, +1 leading <EOS> token for decoder inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder without projection.\n",
    "Internal transition step uses beam search\n",
    "```\n",
    "output(t) -> output projection(t) -> prediction(t) (argmax) -> input projection(t+1) -> next input(t+1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder via `tf.nn.raw_rnn`\n",
    "\n",
    "`tf.nn.dynamic_rnn` allows for easy RNN construction, but is limited. For example, a nice way to increase robustness of the model is to feed as decoder inputs tokens that it previously generated, instead of shifted true sequence.\n",
    "\n",
    "![seq2seq-feed-previous](pictures/2-seq2seq-feed-previous.png)\n",
    "*Image borrowed from http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert EOS == 1\n",
    "assert PAD == 0\n",
    "\n",
    "eos_time_slice = tf.one_hot(\n",
    "    tf.ones([batch_size], \n",
    "            dtype=tf.int32, name='EOS'), \n",
    "    vocab_size, name='EOS_OneHot')\n",
    "\n",
    "pad_time_slice = tf.one_hot(\n",
    "    tf.zeros([batch_size], \n",
    "             dtype=tf.int32, name='PAD'),\n",
    "    vocab_size, name='PAD_OneHot')\n",
    "\n",
    "def loop_fn_initial(time, cell_output, cell_state, loop_state):\n",
    "    assert cell_output is None and loop_state is None and cell_state is None\n",
    "\n",
    "    elements_finished = (time >= decoder_lengths)  # all True at the 1st step\n",
    "    with tf.variable_scope('DecoderInputProjection') as scope:\n",
    "        initial_input = projection(eos_time_slice, input_embedding_size, scope)\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    \n",
    "    return (elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            None,  # cell output is dummy here\n",
    "            initial_loop_state)\n",
    "\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    \"\"\" loop_fn determines transitions between RNN unroll steps\n",
    "    \"\"\"\n",
    "\n",
    "    if cell_state is None:    # time == 0\n",
    "        return loop_fn_initial(time, cell_output, cell_state, loop_state)\n",
    "    \n",
    "    emit_output = cell_output  # == None for time == 0\n",
    "\n",
    "    next_cell_state = cell_state\n",
    "\n",
    "    elements_finished = (time >= decoder_lengths)\n",
    "    finished = tf.reduce_all(elements_finished)\n",
    "\n",
    "    def pad_step():\n",
    "        with tf.variable_scope('DecoderInputProjection', reuse=True) as scope:\n",
    "            return projection(pad_time_slice, input_embedding_size, scope)\n",
    "        \n",
    "    def beam_step():\n",
    "        \"\"\" output->input transition:\n",
    "\n",
    "            output[t] -> output projection[t] -> prediction[t] ->\n",
    "            -> input[t+1] -> input projection[t+1]\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('DecoderOutputProjection') as scope:\n",
    "            output = projection(cell_output, vocab_size, scope)\n",
    "        prediction = tf.argmax(output, axis=1)\n",
    "        prediction_onehot = tf.one_hot(prediction, vocab_size)\n",
    "        with tf.variable_scope('DecoderInputProjection', reuse=True) as scope:\n",
    "            projection_ = projection(prediction_onehot, input_embedding_size, scope)\n",
    "        return projection_\n",
    "    \n",
    "    next_input = tf.cond(finished, pad_step, beam_step)\n",
    "\n",
    "    next_loop_state = None\n",
    "\n",
    "    result = (elements_finished, \n",
    "            next_input, \n",
    "            next_cell_state,\n",
    "            emit_output,\n",
    "            next_loop_state)\n",
    "    \n",
    "    return result\n",
    "\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()\n",
    "\n",
    "with tf.variable_scope('DecoderOutputProjection') as scope:\n",
    "    decoder_logits = projection(decoder_outputs, vocab_size, scope)\n",
    "\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN outputs tensor of shape `[max_time, batch_size, hidden_units]` which projection layer maps onto `[max_time, batch_size, vocab_size]`. `vocab_size` part of the shape is static, while `max_time` and `batch_size` is dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the toy task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the copy task — given a random sequence of integers from a `vocabulary`, learn to memorize and reproduce input sequence. Because sequences are random, they do not contain any structure, unlike natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[2, 8, 5, 2, 2, 5, 7, 7]\n",
      "[4, 8, 7, 7]\n",
      "[4, 7, 8, 4, 5]\n",
      "[2, 3, 8]\n",
      "[8, 2, 4, 2, 5, 6, 2, 8]\n",
      "[9, 7, 6, 2, 4, 6, 8, 7]\n",
      "[9, 7, 3, 8]\n",
      "[7, 7, 7, 5, 8, 9, 8, 6]\n",
      "[3, 2, 6, 2, 3, 9, 7]\n",
      "[4, 5, 8, 6]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.294760227203369\n",
      "  sample 1:\n",
      "    input     > [7 4 7 3 0 0 0 0]\n",
      "    predicted > [5 5 8 8 8 8 5 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 7 7 9 5 0 0 0]\n",
      "    predicted > [7 7 7 5 5 3 3 5 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 9 5 4 4 8 5 8]\n",
      "    predicted > [2 3 3 3 3 3 3 3 5 5 9]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.6675865054130554\n",
      "  sample 1:\n",
      "    input     > [4 2 9 9 7 2 0 0]\n",
      "    predicted > [4 2 9 9 7 2 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 4 7 0 0 0 0 0]\n",
      "    predicted > [8 4 7 1 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 9 3 6 3 7 2 0]\n",
      "    predicted > [5 5 3 3 3 7 2 1 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.3265087604522705\n",
      "  sample 1:\n",
      "    input     > [4 3 8 9 9 9 0 0]\n",
      "    predicted > [4 3 8 9 9 9 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 7 6 6 0 0 0 0]\n",
      "    predicted > [4 7 6 6 1 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 7 9 3 8 3 8 0]\n",
      "    predicted > [5 7 9 3 3 3 8 1 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.16439595818519592\n",
      "  sample 1:\n",
      "    input     > [5 9 7 6 8 8 0 0]\n",
      "    predicted > [5 9 7 6 8 8 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 2 5 6 9 9 5 0]\n",
      "    predicted > [2 5 5 9 9 9 5 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 9 5 3 2 7 8 0]\n",
      "    predicted > [3 9 5 3 2 7 8 1 0 0 0]\n",
      "\n",
      "batch 4000\n",
      "  minibatch loss: 0.12039782106876373\n",
      "  sample 1:\n",
      "    input     > [5 9 7 6 5 5 3 6]\n",
      "    predicted > [5 9 7 9 5 5 3 6 1 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 7 6 5 7 6 5 0]\n",
      "    predicted > [7 7 6 7 7 6 5 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 9 6 0 0 0 0 0]\n",
      "    predicted > [9 9 6 1 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 5000\n",
      "  minibatch loss: 0.08047980815172195\n",
      "  sample 1:\n",
      "    input     > [5 8 3 0 0 0 0 0]\n",
      "    predicted > [5 8 3 1 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 4 3 9 6 0 0 0]\n",
      "    predicted > [5 4 3 9 6 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 7 6 0 0 0 0 0]\n",
      "    predicted > [6 7 6 1 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 6000\n",
      "  minibatch loss: 0.04571397230029106\n",
      "  sample 1:\n",
      "    input     > [3 5 9 2 4 0 0 0]\n",
      "    predicted > [3 5 9 2 4 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 7 9 7 8 9 0 0]\n",
      "    predicted > [5 7 9 7 8 9 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 2 3 9 0 0 0 0]\n",
      "    predicted > [8 2 3 9 1 0 0 0 0 0 0]\n",
      "\n",
      "batch 7000\n",
      "  minibatch loss: 0.02951410412788391\n",
      "  sample 1:\n",
      "    input     > [9 5 7 8 0 0 0 0]\n",
      "    predicted > [9 5 7 8 1 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 8 4 6 4 4 0 0]\n",
      "    predicted > [6 8 4 6 4 4 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 5 5 9 8 6 5 0]\n",
      "    predicted > [6 5 5 9 8 6 5 1 0 0 0]\n",
      "\n",
      "batch 8000\n",
      "  minibatch loss: 0.0316622368991375\n",
      "  sample 1:\n",
      "    input     > [2 6 4 5 0 0 0 0]\n",
      "    predicted > [2 6 4 5 1 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 4 6 6 9 2 0 0]\n",
      "    predicted > [9 4 6 6 9 2 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 9 2 4 0 0 0 0]\n",
      "    predicted > [4 9 2 4 1 0 0 0 0 0 0]\n",
      "\n",
      "batch 9000\n",
      "  minibatch loss: 0.017353234812617302\n",
      "  sample 1:\n",
      "    input     > [7 9 6 3 6 0 0 0]\n",
      "    predicted > [7 9 6 3 6 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 8 3 4 7 6 2 0]\n",
      "    predicted > [7 8 3 4 7 6 2 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 5 5 4 8 0 0 0]\n",
      "    predicted > [2 5 5 4 8 1 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 10000\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0120 after 1000000 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLDcWMAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f436630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
