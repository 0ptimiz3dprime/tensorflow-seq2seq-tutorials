{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "# Playing with new 2017 `tf.contrib.seq2seq`\n",
    "\n",
    "I just discovered that a [new dynamic seq2seq implementation](https://github.com/tensorflow/tensorflow/tree/24466c2e6d32621cd85f0a78d47df6eed2c5c5a6/tensorflow/contrib/seq2seq) was recently merged into master and will be released with `TF 1.0.0`. Naturally, I wanted to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, GRUCell\n",
    "from model_new import Seq2SeqModel, train_on_copy_task\n",
    "import pandas as pd\n",
    "import helpers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point implementations are quite long, so I put them in [model_new.py](model_new.py), while notebook will illustrate the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # with bidirectional encoder, decoder state size should be\n",
    "    # 2x encoder state size\n",
    "    model = Seq2SeqModel(encoder_cell=LSTMCell(10),\n",
    "                         decoder_cell=LSTMCell(20), \n",
    "                         vocab_size=10,\n",
    "                         embedding_size=10,\n",
    "                         attention=True,\n",
    "                         bidirectional=True,\n",
    "                         debug=False)\n",
    "\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    train_on_copy_task(session, model,\n",
    "                       length_from=3, length_to=8,\n",
    "                       vocab_lower=2, vocab_upper=10,\n",
    "                       batch_size=100,\n",
    "                       max_batches=3000,\n",
    "                       batches_in_epoch=1000,\n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun exercise, compare performance of different seq2seq variants.\n",
    "\n",
    "Comparison will be done using train loss tracks, since the task is algorithmic and data is generated directly from true distribution and out-of-sample testing doesn't really make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_tracks = dict()\n",
    "\n",
    "def do_train(session, model):\n",
    "    return train_on_copy_task(session, model,\n",
    "                              length_from=3, length_to=8,\n",
    "                              vocab_lower=2, vocab_upper=10,\n",
    "                              batch_size=100,\n",
    "                              max_batches=5000,\n",
    "                              batches_in_epoch=1000,\n",
    "                              verbose=False)\n",
    "\n",
    "def make_model(**kwa):\n",
    "    args = dict(cell_class=LSTMCell,\n",
    "                num_units_encoder=10,\n",
    "                vocab_size=10,\n",
    "                embedding_size=10,\n",
    "                attention=False,\n",
    "                bidirectional=False,\n",
    "                debug=False)\n",
    "    args.update(kwa)\n",
    "    \n",
    "    cell_class = args.pop('cell_class')\n",
    "    \n",
    "    num_units_encoder = args.pop('num_units_encoder')\n",
    "    num_units_decoder = num_units_encoder\n",
    "    \n",
    "    if args['bidirectional']:\n",
    "        num_units_decoder *= 2\n",
    "    \n",
    "    args['encoder_cell'] = cell_class(num_units_encoder)\n",
    "    args['decoder_cell'] = cell_class(num_units_decoder)\n",
    "    \n",
    "    return Seq2SeqModel(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test bidirectional/forward encoder, attention/no attention, in all combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "with tf.Session() as session:\n",
    "    model = make_model(bidirectional=False, attention=False)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    loss_tracks['forward encoder, no attention'] = do_train(session, model)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "with tf.Session() as session:\n",
    "    model = make_model(bidirectional=True, attention=False)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    loss_tracks['bidirectional encoder, no attention'] = do_train(session, model)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "with tf.Session() as session:\n",
    "    model = make_model(bidirectional=False, attention=True)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    loss_tracks['forward encoder, with attention'] = do_train(session, model)\n",
    "\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "with tf.Session() as session:\n",
    "    model = make_model(bidirectional=True, attention=True)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    loss_tracks['bidirectional encoder, with attention'] = do_train(session, model)\n",
    "\n",
    "pd.DataFrame(loss_tracks).plot(figsize=(13, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, attention helps a lot when the task is simply copying from inputs to outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test GRU vs LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "with tf.Session() as session:\n",
    "    model = make_model(bidirectional=True, attention=True, cell_class=LSTMCell)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    t0 = time.time()\n",
    "    lstm_track = do_train(session, model)\n",
    "    lstm_took = time.time() - t0\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "with tf.Session() as session:\n",
    "    model = make_model(bidirectional=True, attention=True, cell_class=GRUCell)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    t0 = time.time()\n",
    "    gru_track = do_train(session, model)\n",
    "    gru_took = time.time() - t0\n",
    "    \n",
    "gru = pd.Series(gru_track, name='gru')\n",
    "lstm = pd.Series(lstm_track, name='lstm')\n",
    "tracks_batch = pd.DataFrame(dict(lstm=lstm, gru=gru))\n",
    "tracks_batch.index.name = 'batch'\n",
    "\n",
    "gru.index = gru.index / gru_took\n",
    "lstm.index = lstm.index / lstm_took\n",
    "tracks_time = pd.DataFrame(dict(lstm=lstm, gru=gru)).ffill()\n",
    "tracks_time.index.name = 'time (seconds)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracks_batch.plot(figsize=(8, 5), title='GRU vs LSTM loss, batch-time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracks_batch.plot(figsize=(8, 5), title='GRU vs LSTM loss, compute-time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU has fewer parameters, so training supposed to be faster? This test doesn't show it."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
