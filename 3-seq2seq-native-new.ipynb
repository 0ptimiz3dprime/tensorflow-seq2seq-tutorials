{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "# Playing with new 2017 `tf.contrib.seq2seq`\n",
    "\n",
    "I just discovered that a [new dynamic seq2seq implementation](https://github.com/tensorflow/tensorflow/tree/24466c2e6d32621cd85f0a78d47df6eed2c5c5a6/tensorflow/contrib/seq2seq) was recently merged into master. Naturally, I wanted to try it out.\n",
    "\n",
    "`Working with commit 24466c2e6d32621cd85f0a78d47df6eed2c5c5a6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "outputExpanded": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0-alpha'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 3\n",
    "decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is time-major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## debugger inputs\n",
    "\n",
    "# x = [[5, 6, 7],\n",
    "#      [7, 6, 0],\n",
    "#      [0, 7, 0]]\n",
    "# xl = [2, 3, 1]\n",
    "# encoder_inputs = tf.constant(x, dtype=tf.int32, name='encoder_inputs')\n",
    "# encoder_inputs_length = tf.constant(xl, dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "# decoder_targets = tf.constant(x, dtype=tf.int32, name='decoder_targets')\n",
    "# decoder_targets_length = tf.constant(xl, dtype=tf.int32, name='decoder_targets_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "decoder_targets_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='decoder_targets_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, `decoder_targets` would serve as basis for both `decoder_inputs` and decoder logits. This means that their shapes should be compatible.\n",
    "\n",
    "Here we do a bit of plumbing to set this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_size, batch_size = tf.unstack(tf.shape(decoder_targets))\n",
    "\n",
    "EOS_SLICE = tf.ones([1, batch_size], dtype=tf.int32)\n",
    "PAD_SLICE = tf.zeros([1, batch_size], dtype=tf.int32)\n",
    "\n",
    "decoder_train_inputs = tf.concat_v2([EOS_SLICE, decoder_targets], axis=0)\n",
    "decoder_train_length = decoder_targets_length + 1\n",
    "\n",
    "decoder_train_targets = tf.concat_v2([decoder_targets, PAD_SLICE], axis=0)\n",
    "decoder_train_targets_seqlen, _ = tf.unstack(tf.shape(decoder_train_targets))\n",
    "decoder_train_targets_eos_mask = tf.transpose(tf.one_hot(decoder_train_length - 1, decoder_train_targets_seqlen, dtype=tf.int32), [1, 0])\n",
    "decoder_train_targets = tf.add(\n",
    "    decoder_train_targets,\n",
    "    decoder_train_targets_eos_mask,\n",
    ")  # hacky way to put EOS symbol at the end of target sequence\n",
    "\n",
    "loss_weights = tf.ones([batch_size, tf.reduce_max(decoder_train_length)], dtype=tf.float32, name=\"loss_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# oh=tf.one_hot(decoder_train_length, decoder_train_targets_seqlen, dtype=tf.int32)\n",
    "\n",
    "# sess.run([oh, decoder_train_targets_eos_mask], fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import (LSTMCell, LSTMStateTuple, EmbeddingWrapper)\n",
    "import tensorflow.contrib.seq2seq as seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.contrib.layers import embedding_lookup_unique\n",
    "\n",
    "with tf.variable_scope(\"embedding\") as scope:\n",
    "\n",
    "    # Uniform(-sqrt(3), sqrt(3)) has variance=1.\n",
    "    sqrt3 = math.sqrt(3)\n",
    "    initializer = tf.random_uniform_initializer(-sqrt3, sqrt3)\n",
    "\n",
    "    embedding_matrix = tf.get_variable(\n",
    "        name=\"embedding_matrix\",\n",
    "        shape=[vocab_size, input_embedding_size],\n",
    "        initializer=initializer,\n",
    "        dtype=tf.float32)\n",
    "    \n",
    "    encoder_inputs_embedded = embedding_lookup_unique(embedding_matrix, encoder_inputs)\n",
    "    \n",
    "    decoder_train_inputs_embedded = embedding_lookup_unique(embedding_matrix, decoder_train_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\") as scope:\n",
    "    encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "    \n",
    "    ((encoder_fw_outputs,\n",
    "      encoder_bw_outputs),\n",
    "     (encoder_fw_state,\n",
    "      encoder_bw_state)) = (\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                        cell_bw=encoder_cell,\n",
    "                                        inputs=encoder_inputs_embedded,\n",
    "                                        sequence_length=encoder_inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True, scope=scope)\n",
    "        )\n",
    "\n",
    "    encoder_outputs = tf.concat_v2((encoder_fw_outputs, encoder_fw_outputs), 2)\n",
    "\n",
    "    encoder_state_c = tf.concat_v2(\n",
    "        (encoder_fw_state.c, encoder_bw_state.c), 1)\n",
    "\n",
    "    encoder_state_h = tf.concat_v2(\n",
    "        (encoder_fw_state.h, encoder_bw_state.h), 1)\n",
    "\n",
    "    encoder_state = LSTMStateTuple(\n",
    "        c=encoder_state_c,\n",
    "        h=encoder_state_h\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple decoder without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "\n",
    "# with tf.variable_scope(\"Decoder\") as scope:\n",
    "#     decoder_fn_train = seq2seq.simple_decoder_fn_train(encoder_state=encoder_state)\n",
    "    \n",
    "#     (\n",
    "#         decoder_outputs_train,\n",
    "#         decoder_state_train,\n",
    "#         decoder_context_state_train\n",
    "#     ) = (\n",
    "#         seq2seq.dynamic_rnn_decoder(\n",
    "#            cell=decoder_cell,\n",
    "#            decoder_fn=decoder_fn_train,\n",
    "#            inputs=decoder_train_inputs_embedded,\n",
    "#            sequence_length=decoder_train_length,\n",
    "#            time_major=True,\n",
    "#            scope=scope,\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     def output_fn(outputs):\n",
    "#         return tf.contrib.layers.linear(outputs, vocab_size, scope=scope)\n",
    "    \n",
    "#     decoder_logits_train = output_fn(decoder_outputs_train)\n",
    "    \n",
    "#     decoder_prediction_train = tf.argmax(decoder_logits_train, axis=-1, name='decoder_prediction')\n",
    "    \n",
    "#     scope.reuse_variables()\n",
    "    \n",
    "#     decoder_fn_inference = seq2seq.simple_decoder_fn_inference(\n",
    "#         output_fn=output_fn,\n",
    "#         encoder_state=encoder_state,\n",
    "#         embeddings=embedding_matrix,\n",
    "#         start_of_sequence_id=1,\n",
    "#         end_of_sequence_id=1,\n",
    "#         maximum_length=tf.reduce_max(encoder_inputs_length) + 3,\n",
    "#         num_decoder_symbols=vocab_size,\n",
    "#     )\n",
    "    \n",
    "#     (\n",
    "#         decoder_outputs_inference,\n",
    "#         decoder_state_inference,\n",
    "#         decoder_context_state_inference\n",
    "#     ) = (\n",
    "#         seq2seq.dynamic_rnn_decoder(\n",
    "#            cell=decoder_cell,\n",
    "#            decoder_fn=decoder_fn_inference,\n",
    "#            time_major=True,\n",
    "#            scope=scope,\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     ## @TODO: in case of inference decoder_outputs_inference is logits, not cell_output?\n",
    "\n",
    "\n",
    "# seqloss = seq2seq.sequence_loss(\n",
    "#     logits=tf.transpose(decoder_logits_train, [1, 0, 2]),\n",
    "#     targets=tf.transpose(decoder_train_targets, [1, 0]),\n",
    "#     weights=loss_weights)\n",
    "\n",
    "# loss = tf.reduce_mean(seqloss)\n",
    "# train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "\n",
    "with tf.variable_scope(\"Decoder\") as scope:\n",
    "    (attention_keys,\n",
    "     attention_values,\n",
    "     attention_score_fn,\n",
    "     attention_construct_fn) = seq2seq.prepare_attention(\n",
    "        attention_states=encoder_state.h, \n",
    "        attention_option=\"bahdanau\", \n",
    "        num_units=decoder_hidden_units,\n",
    "    )\n",
    "    \n",
    "    decoder_fn_train = seq2seq.attention_decoder_fn_train(\n",
    "        encoder_state=encoder_state,\n",
    "        attention_keys=attention_keys,\n",
    "        attention_values=attention_values,\n",
    "        attention_score_fn=attention_score_fn,\n",
    "        attention_construct_fn=attention_construct_fn,\n",
    "        name='attention_decoder'\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        decoder_outputs_train,\n",
    "        decoder_state_train,\n",
    "        decoder_context_state_train\n",
    "    ) = (\n",
    "        seq2seq.dynamic_rnn_decoder(\n",
    "           cell=decoder_cell,\n",
    "           decoder_fn=decoder_fn_train,\n",
    "           inputs=decoder_train_inputs_embedded,\n",
    "           sequence_length=decoder_train_length,\n",
    "           time_major=True,\n",
    "           scope=scope,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    def output_fn(outputs):\n",
    "        return tf.contrib.layers.linear(outputs, vocab_size, scope=scope)\n",
    "    \n",
    "    decoder_logits_train = output_fn(decoder_outputs_train)\n",
    "    \n",
    "    decoder_prediction_train = tf.argmax(decoder_logits_train, axis=-1, name='decoder_prediction')\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    \n",
    "    decoder_fn_inference = seq2seq.attention_decoder_fn_inference(\n",
    "        output_fn=output_fn,\n",
    "        encoder_state=encoder_state,\n",
    "        attention_keys=attention_keys,\n",
    "        attention_values=attention_values,\n",
    "        attention_score_fn=attention_score_fn,\n",
    "        attention_construct_fn=attention_construct_fn,\n",
    "        embeddings=embedding_matrix,\n",
    "        start_of_sequence_id=1,\n",
    "        end_of_sequence_id=1,\n",
    "        maximum_length=tf.reduce_max(encoder_inputs_length) + 3,\n",
    "        num_decoder_symbols=vocab_size,\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        decoder_outputs_inference,\n",
    "        decoder_state_inference,\n",
    "        decoder_context_state_inference\n",
    "    ) = (\n",
    "        seq2seq.dynamic_rnn_decoder(\n",
    "           cell=decoder_cell,\n",
    "           decoder_fn=decoder_fn_inference,\n",
    "           time_major=True,\n",
    "           scope=scope,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    ## @TODO: in case of inference decoder_outputs_inference is logits, not cell_output?\n",
    "\n",
    "\n",
    "seqloss = seq2seq.sequence_loss(\n",
    "    logits=tf.transpose(decoder_logits_train, [1, 0, 2]),\n",
    "    targets=tf.transpose(decoder_train_targets, [1, 0]),\n",
    "    weights=loss_weights)\n",
    "\n",
    "loss = tf.reduce_mean(seqloss)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2, 5, 9, 8, 7]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "seq = next(batches)[0]\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    inputs_, inputs_length_ = helpers.batch(batch)\n",
    "    return {\n",
    "        encoder_inputs: inputs_,\n",
    "        encoder_inputs_length: inputs_length_,\n",
    "        decoder_targets: inputs_,\n",
    "        decoder_targets_length: inputs_length_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"encoder_inputs:0\", shape=(?, ?), dtype=int32) 8\n",
      "Tensor(\"encoder_inputs_length:0\", shape=(?,), dtype=int32) 100\n",
      "Tensor(\"decoder_targets:0\", shape=(?, ?), dtype=int32) 8\n",
      "Tensor(\"decoder_targets_length:0\", shape=(?,), dtype=int32) 100\n"
     ]
    }
   ],
   "source": [
    "for k, v in next_feed().items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 0.7591437697410583\n",
      "  sample 1:\n",
      "    enc input           > [6 6 4 2 6 4 0 0]\n",
      "    dec train predicted > [4 4 4 2 6 4 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [9 2 6 9 7 0 0 0]\n",
      "    dec train predicted > [9 2 9 9 7 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 5 3 0 0 0 0 0]\n",
      "    dec train predicted > [5 5 3 1 0 0 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.5976548194885254\n",
      "  sample 1:\n",
      "    enc input           > [2 5 3 6 0 0 0 0]\n",
      "    dec train predicted > [2 6 3 6 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [9 7 5 3 7 7 6 0]\n",
      "    dec train predicted > [9 7 9 3 7 5 9 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [4 7 3 5 6 9 3 5]\n",
      "    dec train predicted > [4 9 9 5 9 9 9 9 1]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.5280476808547974\n",
      "  sample 1:\n",
      "    enc input           > [9 9 8 7 5 5 6 7]\n",
      "    dec train predicted > [9 5 8 7 5 5 8 2 1]\n",
      "  sample 2:\n",
      "    enc input           > [8 7 6 7 0 0 0 0]\n",
      "    dec train predicted > [8 7 6 7 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 3 8 7 0 0 0 0]\n",
      "    dec train predicted > [7 3 8 7 1 0 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.4008585512638092\n",
      "  sample 1:\n",
      "    enc input           > [6 2 3 8 0 0 0 0]\n",
      "    dec train predicted > [6 2 3 8 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [2 9 5 3 0 0 0 0]\n",
      "    dec train predicted > [6 9 6 3 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [6 9 6 5 9 5 8 2]\n",
      "    dec train predicted > [9 9 5 5 5 8 8 2 1]\n",
      "\n",
      "batch 4000\n",
      "  minibatch loss: 0.41999635100364685\n",
      "  sample 1:\n",
      "    enc input           > [4 3 2 7 2 2 3 0]\n",
      "    dec train predicted > [4 4 2 4 2 2 3 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [6 2 6 5 6 0 0 0]\n",
      "    dec train predicted > [6 2 6 5 6 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [8 5 9 5 7 8 7 4]\n",
      "    dec train predicted > [8 5 9 5 7 8 4 4 1]\n",
      "\n",
      "batch 5000\n",
      "  minibatch loss: 0.3452256917953491\n",
      "  sample 1:\n",
      "    enc input           > [4 5 2 2 6 8 9 0]\n",
      "    dec train predicted > [4 8 2 2 5 5 9 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [2 2 2 7 0 0 0 0]\n",
      "    dec train predicted > [2 2 2 7 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [4 9 4 5 5 7 0 0]\n",
      "    dec train predicted > [4 9 7 5 7 7 1 0 0]\n",
      "\n",
      "batch 6000\n",
      "  minibatch loss: 0.32028213143348694\n",
      "  sample 1:\n",
      "    enc input           > [6 2 9 7 8 0 0 0]\n",
      "    dec train predicted > [6 2 9 5 8 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [6 9 2 8 6 3 6 8]\n",
      "    dec train predicted > [6 6 2 8 3 3 8 8 1]\n",
      "  sample 3:\n",
      "    enc input           > [2 8 4 9 0 0 0 0]\n",
      "    dec train predicted > [2 8 4 9 1 0 0 0 0]\n",
      "\n",
      "batch 7000\n",
      "  minibatch loss: 0.32483986020088196\n",
      "  sample 1:\n",
      "    enc input           > [9 7 6 5 5 0 0 0]\n",
      "    dec train predicted > [9 5 6 5 5 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [9 4 2 0 0 0 0 0]\n",
      "    dec train predicted > [3 4 2 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [4 5 6 0 0 0 0 0]\n",
      "    dec train predicted > [4 5 6 1 0 0 0 0 0]\n",
      "\n",
      "batch 8000\n",
      "  minibatch loss: 0.2594529092311859\n",
      "  sample 1:\n",
      "    enc input           > [3 8 3 7 8 0 0 0]\n",
      "    dec train predicted > [3 8 3 7 8 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [9 2 2 0 0 0 0 0]\n",
      "    dec train predicted > [6 2 2 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [6 6 5 8 3 8 0 0]\n",
      "    dec train predicted > [6 6 5 8 3 8 1 0 0]\n",
      "\n",
      "batch 9000\n",
      "  minibatch loss: 0.2642223834991455\n",
      "  sample 1:\n",
      "    enc input           > [6 6 7 3 8 0 0 0]\n",
      "    dec train predicted > [6 6 7 3 8 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [5 8 4 3 4 9 0 0]\n",
      "    dec train predicted > [5 2 4 3 4 9 1 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [6 9 4 0 0 0 0 0]\n",
      "    dec train predicted > [6 9 4 1 0 0 0 0 0]\n",
      "\n",
      "batch 10000\n",
      "  minibatch loss: 0.25036245584487915\n",
      "  sample 1:\n",
      "    enc input           > [5 2 8 5 3 5 0 0]\n",
      "    dec train predicted > [8 2 8 5 6 5 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [8 4 4 5 2 3 7 0]\n",
      "    dec train predicted > [8 4 4 5 2 3 7 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [2 3 3 5 9 9 2 0]\n",
      "    dec train predicted > [2 3 9 5 9 6 2 1 0]\n",
      "\n",
      "batch 11000\n",
      "  minibatch loss: 0.18412484228610992\n",
      "  sample 1:\n",
      "    enc input           > [5 9 9 6 4 7 0 0]\n",
      "    dec train predicted > [9 9 9 6 4 7 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [6 6 5 4 2 0 0 0]\n",
      "    dec train predicted > [6 6 8 2 2 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [8 7 7 3 4 9 0 0]\n",
      "    dec train predicted > [5 7 7 3 3 9 1 0 0]\n",
      "\n",
      "batch 12000\n",
      "  minibatch loss: 0.24699769914150238\n",
      "  sample 1:\n",
      "    enc input           > [3 9 5 6 0 0 0 0]\n",
      "    dec train predicted > [3 9 5 6 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [7 5 2 0 0 0 0 0]\n",
      "    dec train predicted > [7 8 2 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [9 8 7 7 8 8 2 6]\n",
      "    dec train predicted > [5 8 7 7 8 8 2 6 1]\n",
      "\n",
      "batch 13000\n",
      "  minibatch loss: 0.20244760811328888\n",
      "  sample 1:\n",
      "    enc input           > [9 5 3 2 8 0 0 0]\n",
      "    dec train predicted > [9 5 3 2 8 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 6 5 0 0 0 0 0]\n",
      "    dec train predicted > [3 6 5 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 9 5 5 6 4 2 0]\n",
      "    dec train predicted > [7 9 5 5 4 4 2 1 0]\n",
      "\n",
      "batch 14000\n",
      "  minibatch loss: 0.1854252815246582\n",
      "  sample 1:\n",
      "    enc input           > [2 2 4 7 7 2 5 0]\n",
      "    dec train predicted > [2 4 4 7 7 2 5 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [8 7 5 0 0 0 0 0]\n",
      "    dec train predicted > [8 7 5 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [4 6 7 4 5 8 0 0]\n",
      "    dec train predicted > [4 6 7 4 5 8 1 0 0]\n",
      "\n",
      "batch 15000\n",
      "  minibatch loss: 0.1984817385673523\n",
      "  sample 1:\n",
      "    enc input           > [6 4 3 4 0 0 0 0]\n",
      "    dec train predicted > [6 4 3 4 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 2 6 3 2 0 0 0]\n",
      "    dec train predicted > [3 2 6 3 2 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [5 4 8 4 7 9 6 5]\n",
      "    dec train predicted > [7 4 8 4 7 9 6 5 1]\n",
      "\n",
      "batch 16000\n",
      "  minibatch loss: 0.20304185152053833\n",
      "  sample 1:\n",
      "    enc input           > [3 7 4 9 9 4 4 3]\n",
      "    dec train predicted > [3 7 3 9 9 4 4 3 1]\n",
      "  sample 2:\n",
      "    enc input           > [9 3 2 4 4 2 9 4]\n",
      "    dec train predicted > [9 3 2 4 4 2 9 4 1]\n",
      "  sample 3:\n",
      "    enc input           > [4 3 5 3 9 3 0 0]\n",
      "    dec train predicted > [4 3 9 3 9 3 1 0 0]\n",
      "\n",
      "batch 17000\n",
      "  minibatch loss: 0.231374591588974\n",
      "  sample 1:\n",
      "    enc input           > [7 2 9 9 7 0 0 0]\n",
      "    dec train predicted > [7 6 9 9 7 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [7 7 6 0 0 0 0 0]\n",
      "    dec train predicted > [7 7 6 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [6 7 5 4 3 0 0 0]\n",
      "    dec train predicted > [6 7 5 4 3 1 0 0 0]\n",
      "\n",
      "batch 18000\n",
      "  minibatch loss: 0.15322791039943695\n",
      "  sample 1:\n",
      "    enc input           > [3 8 9 5 0 0 0 0]\n",
      "    dec train predicted > [3 8 9 5 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [4 8 7 3 0 0 0 0]\n",
      "    dec train predicted > [4 8 7 3 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [8 4 4 6 0 0 0 0]\n",
      "    dec train predicted > [2 4 4 6 1 0 0 0 0]\n",
      "\n",
      "batch 19000\n",
      "  minibatch loss: 0.1715458631515503\n",
      "  sample 1:\n",
      "    enc input           > [5 3 8 0 0 0 0 0]\n",
      "    dec train predicted > [5 3 8 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [4 5 8 7 5 3 2 5]\n",
      "    dec train predicted > [7 5 8 7 7 4 2 2 1]\n",
      "  sample 3:\n",
      "    enc input           > [5 6 7 8 9 2 4 0]\n",
      "    dec train predicted > [5 6 7 8 5 2 4 1 0]\n",
      "\n",
      "batch 20000\n",
      "  minibatch loss: 0.12816040217876434\n",
      "  sample 1:\n",
      "    enc input           > [6 3 3 8 0 0 0 0]\n",
      "    dec train predicted > [6 3 3 8 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [5 8 7 3 4 9 0 0]\n",
      "    dec train predicted > [5 5 7 3 4 9 1 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 8 5 4 9 2 8 0]\n",
      "    dec train predicted > [7 8 5 4 5 2 8 1 0]\n",
      "\n",
      "batch 21000\n",
      "  minibatch loss: 0.16078223288059235\n",
      "  sample 1:\n",
      "    enc input           > [5 4 3 0 0 0 0 0]\n",
      "    dec train predicted > [5 4 3 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [4 3 2 8 4 2 6 0]\n",
      "    dec train predicted > [4 3 2 2 4 2 6 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [4 6 2 4 5 9 4 2]\n",
      "    dec train predicted > [4 6 2 4 5 3 2 2 1]\n",
      "\n",
      "batch 22000\n",
      "  minibatch loss: 0.16905421018600464\n",
      "  sample 1:\n",
      "    enc input           > [3 4 8 9 7 7 0 0]\n",
      "    dec train predicted > [3 4 5 9 7 7 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [8 7 3 6 9 9 8 4]\n",
      "    dec train predicted > [8 7 3 6 9 5 8 4 1]\n",
      "  sample 3:\n",
      "    enc input           > [8 2 2 8 0 0 0 0]\n",
      "    dec train predicted > [8 2 2 8 1 0 0 0 0]\n",
      "\n",
      "batch 23000\n",
      "  minibatch loss: 0.1547243893146515\n",
      "  sample 1:\n",
      "    enc input           > [4 3 8 5 5 2 8 3]\n",
      "    dec train predicted > [4 3 8 5 8 2 8 7 1]\n",
      "  sample 2:\n",
      "    enc input           > [3 9 8 9 5 4 0 0]\n",
      "    dec train predicted > [3 9 8 9 5 4 1 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [3 4 5 5 4 2 0 0]\n",
      "    dec train predicted > [3 4 5 7 4 2 1 0 0]\n",
      "\n",
      "batch 24000\n",
      "  minibatch loss: 0.15996231138706207\n",
      "  sample 1:\n",
      "    enc input           > [6 8 8 0 0 0 0 0]\n",
      "    dec train predicted > [6 8 8 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [7 2 5 6 3 3 6 0]\n",
      "    dec train predicted > [7 2 9 6 3 3 6 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [8 8 9 9 4 4 0 0]\n",
      "    dec train predicted > [8 5 9 9 4 4 1 0 0]\n",
      "\n",
      "batch 25000\n",
      "  minibatch loss: 0.14014606177806854\n",
      "  sample 1:\n",
      "    enc input           > [7 2 5 7 6 5 4 5]\n",
      "    dec train predicted > [7 2 5 7 6 5 2 5 1]\n",
      "  sample 2:\n",
      "    enc input           > [4 7 3 2 6 0 0 0]\n",
      "    dec train predicted > [4 7 3 2 6 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [9 8 3 0 0 0 0 0]\n",
      "    dec train predicted > [9 8 3 1 0 0 0 0 0]\n",
      "\n",
      "batch 26000\n",
      "  minibatch loss: 0.21149705350399017\n",
      "  sample 1:\n",
      "    enc input           > [7 7 5 6 0 0 0 0]\n",
      "    dec train predicted > [7 7 5 6 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [8 2 6 6 5 0 0 0]\n",
      "    dec train predicted > [8 2 6 6 5 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [4 9 3 0 0 0 0 0]\n",
      "    dec train predicted > [4 9 3 1 0 0 0 0 0]\n",
      "\n",
      "batch 27000\n",
      "  minibatch loss: 0.13680554926395416\n",
      "  sample 1:\n",
      "    enc input           > [6 9 4 8 7 8 7 8]\n",
      "    dec train predicted > [6 9 4 8 7 5 7 8 1]\n",
      "  sample 2:\n",
      "    enc input           > [4 6 9 0 0 0 0 0]\n",
      "    dec train predicted > [4 6 9 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [4 4 8 7 0 0 0 0]\n",
      "    dec train predicted > [4 4 8 7 1 0 0 0 0]\n",
      "\n",
      "batch 28000\n",
      "  minibatch loss: 0.12651528418064117\n",
      "  sample 1:\n",
      "    enc input           > [3 9 6 3 2 4 0 0]\n",
      "    dec train predicted > [3 9 6 3 2 4 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [7 3 6 5 6 8 4 0]\n",
      "    dec train predicted > [7 3 6 8 6 8 4 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [5 5 4 3 2 8 9 9]\n",
      "    dec train predicted > [5 5 4 3 2 8 9 9 1]\n",
      "\n",
      "batch 29000\n",
      "  minibatch loss: 0.15152613818645477\n",
      "  sample 1:\n",
      "    enc input           > [5 8 5 0 0 0 0 0]\n",
      "    dec train predicted > [5 8 5 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [2 9 3 4 7 3 5 8]\n",
      "    dec train predicted > [2 9 3 4 7 3 5 6 1]\n",
      "  sample 3:\n",
      "    enc input           > [9 5 5 8 3 2 0 0]\n",
      "    dec train predicted > [9 5 5 8 3 2 1 0 0]\n",
      "\n",
      "batch 30000\n",
      "  minibatch loss: 0.14512711763381958\n",
      "  sample 1:\n",
      "    enc input           > [3 7 9 8 2 0 0 0]\n",
      "    dec train predicted > [3 7 9 8 2 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [2 9 5 0 0 0 0 0]\n",
      "    dec train predicted > [2 9 5 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [5 6 8 2 7 3 0 0]\n",
      "    dec train predicted > [5 6 8 2 7 3 1 0 0]\n",
      "\n",
      "batch 31000\n",
      "  minibatch loss: 0.12449871003627777\n",
      "  sample 1:\n",
      "    enc input           > [6 6 5 9 0 0 0 0]\n",
      "    dec train predicted > [6 6 5 9 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [9 8 4 3 8 2 0 0]\n",
      "    dec train predicted > [9 8 4 3 8 2 1 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [8 8 2 5 6 0 0 0]\n",
      "    dec train predicted > [8 8 2 5 6 1 0 0 0]\n",
      "\n",
      "batch 32000\n",
      "  minibatch loss: 0.14141148328781128\n",
      "  sample 1:\n",
      "    enc input           > [9 8 6 2 8 6 9 0]\n",
      "    dec train predicted > [9 8 6 2 5 9 9 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [7 3 4 4 7 3 2 6]\n",
      "    dec train predicted > [7 3 4 4 4 3 2 6 1]\n",
      "  sample 3:\n",
      "    enc input           > [4 9 6 7 4 4 5 5]\n",
      "    dec train predicted > [4 9 3 7 4 4 5 5 1]\n",
      "\n",
      "batch 33000\n",
      "  minibatch loss: 0.16362452507019043\n",
      "  sample 1:\n",
      "    enc input           > [4 7 5 3 5 7 0 0]\n",
      "    dec train predicted > [4 7 5 3 7 7 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 3 8 9 7 7 0 0]\n",
      "    dec train predicted > [3 3 5 9 7 7 1 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 7 2 6 5 0 0 0]\n",
      "    dec train predicted > [7 7 2 6 5 1 0 0 0]\n",
      "\n",
      "batch 34000\n",
      "  minibatch loss: 0.24359427392482758\n",
      "  sample 1:\n",
      "    enc input           > [3 7 9 2 4 9 2 9]\n",
      "    dec train predicted > [3 7 9 2 4 9 8 9 1]\n",
      "  sample 2:\n",
      "    enc input           > [2 6 7 3 7 9 5 4]\n",
      "    dec train predicted > [2 6 7 3 7 5 5 7 1]\n",
      "  sample 3:\n",
      "    enc input           > [5 7 8 4 9 0 0 0]\n",
      "    dec train predicted > [5 7 8 4 9 1 0 0 0]\n",
      "\n",
      "batch 35000\n",
      "  minibatch loss: 0.1336325705051422\n",
      "  sample 1:\n",
      "    enc input           > [4 7 6 2 9 4 0 0]\n",
      "    dec train predicted > [4 5 6 2 9 4 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 8 7 5 6 7 7 0]\n",
      "    dec train predicted > [3 8 7 5 6 7 7 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [6 6 3 6 2 0 0 0]\n",
      "    dec train predicted > [6 6 3 6 2 1 0 0 0]\n",
      "\n",
      "batch 36000\n",
      "  minibatch loss: 0.16005247831344604\n",
      "  sample 1:\n",
      "    enc input           > [3 8 3 0 0 0 0 0]\n",
      "    dec train predicted > [3 8 3 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 8 5 0 0 0 0 0]\n",
      "    dec train predicted > [3 8 5 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 3 6 7 0 0 0 0]\n",
      "    dec train predicted > [7 3 6 7 1 0 0 0 0]\n",
      "\n",
      "batch 37000\n",
      "  minibatch loss: 0.10530677437782288\n",
      "  sample 1:\n",
      "    enc input           > [3 4 5 5 2 2 6 0]\n",
      "    dec train predicted > [3 4 5 5 2 2 6 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [7 2 4 6 0 0 0 0]\n",
      "    dec train predicted > [7 2 4 6 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [2 9 5 7 0 0 0 0]\n",
      "    dec train predicted > [2 9 5 7 1 0 0 0 0]\n",
      "\n",
      "batch 38000\n",
      "  minibatch loss: 0.12887048721313477\n",
      "  sample 1:\n",
      "    enc input           > [6 5 4 6 0 0 0 0]\n",
      "    dec train predicted > [6 5 4 6 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 8 7 7 5 0 0 0]\n",
      "    dec train predicted > [3 5 7 7 5 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 6 2 8 4 5 6 0]\n",
      "    dec train predicted > [7 6 2 8 4 5 6 1 0]\n",
      "\n",
      "batch 39000\n",
      "  minibatch loss: 0.14613932371139526\n",
      "  sample 1:\n",
      "    enc input           > [8 9 8 0 0 0 0 0]\n",
      "    dec train predicted > [8 9 8 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [8 8 6 2 5 4 7 0]\n",
      "    dec train predicted > [8 8 6 2 7 4 7 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [4 9 4 2 7 0 0 0]\n",
      "    dec train predicted > [4 9 4 2 7 1 0 0 0]\n",
      "\n",
      "batch 40000\n",
      "  minibatch loss: 0.09655860811471939\n",
      "  sample 1:\n",
      "    enc input           > [9 9 7 0 0 0 0 0]\n",
      "    dec train predicted > [9 9 7 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [9 5 5 5 8 6 5 0]\n",
      "    dec train predicted > [9 5 5 8 8 6 5 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [3 9 6 0 0 0 0 0]\n",
      "    dec train predicted > [3 9 6 1 0 0 0 0 0]\n",
      "\n",
      "batch 41000\n",
      "  minibatch loss: 0.13763925433158875\n",
      "  sample 1:\n",
      "    enc input           > [7 5 4 4 4 4 3 0]\n",
      "    dec train predicted > [7 7 4 4 4 4 3 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [2 5 3 5 2 0 0 0]\n",
      "    dec train predicted > [2 5 3 5 2 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [3 5 6 8 4 0 0 0]\n",
      "    dec train predicted > [3 5 6 8 4 1 0 0 0]\n",
      "\n",
      "batch 42000\n",
      "  minibatch loss: 0.10303326696157455\n",
      "  sample 1:\n",
      "    enc input           > [3 8 2 4 7 6 0 0]\n",
      "    dec train predicted > [3 8 2 4 7 6 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [8 3 9 0 0 0 0 0]\n",
      "    dec train predicted > [8 3 9 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [8 7 6 0 0 0 0 0]\n",
      "    dec train predicted > [8 7 6 1 0 0 0 0 0]\n",
      "\n",
      "batch 43000\n",
      "  minibatch loss: 0.1151668131351471\n",
      "  sample 1:\n",
      "    enc input           > [6 4 4 5 5 6 0 0]\n",
      "    dec train predicted > [6 4 4 5 5 6 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [2 8 5 6 6 2 8 0]\n",
      "    dec train predicted > [2 8 5 6 6 5 8 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [9 8 7 0 0 0 0 0]\n",
      "    dec train predicted > [9 8 7 1 0 0 0 0 0]\n",
      "\n",
      "batch 44000\n",
      "  minibatch loss: 0.10763672739267349\n",
      "  sample 1:\n",
      "    enc input           > [6 3 8 0 0 0 0 0]\n",
      "    dec train predicted > [6 3 8 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [2 3 6 0 0 0 0 0]\n",
      "    dec train predicted > [2 3 6 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [2 5 6 4 8 3 2 4]\n",
      "    dec train predicted > [2 5 6 4 8 3 2 4 1]\n",
      "\n",
      "batch 45000\n",
      "  minibatch loss: 0.09055892378091812\n",
      "  sample 1:\n",
      "    enc input           > [7 7 7 0 0 0 0 0]\n",
      "    dec train predicted > [7 7 7 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [6 5 6 8 4 8 0 0]\n",
      "    dec train predicted > [6 5 6 8 4 8 1 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [4 6 9 8 9 7 8 0]\n",
      "    dec train predicted > [4 6 9 5 9 7 8 1 0]\n",
      "\n",
      "batch 46000\n",
      "  minibatch loss: 0.08830045908689499\n",
      "  sample 1:\n",
      "    enc input           > [9 5 2 0 0 0 0 0]\n",
      "    dec train predicted > [9 5 2 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [6 2 3 5 2 3 4 0]\n",
      "    dec train predicted > [6 2 3 5 2 3 4 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [4 3 8 7 5 3 8 6]\n",
      "    dec train predicted > [4 3 5 7 5 3 8 6 1]\n",
      "\n",
      "batch 47000\n",
      "  minibatch loss: 0.12990157306194305\n",
      "  sample 1:\n",
      "    enc input           > [4 7 8 7 5 0 0 0]\n",
      "    dec train predicted > [4 7 8 7 5 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [6 2 9 4 0 0 0 0]\n",
      "    dec train predicted > [6 2 9 4 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [3 6 2 6 9 6 0 0]\n",
      "    dec train predicted > [3 6 6 6 9 6 1 0 0]\n",
      "\n",
      "batch 48000\n",
      "  minibatch loss: 0.09437678754329681\n",
      "  sample 1:\n",
      "    enc input           > [3 4 3 6 7 9 3 5]\n",
      "    dec train predicted > [3 4 3 6 7 9 3 5 1]\n",
      "  sample 2:\n",
      "    enc input           > [3 3 8 3 4 2 7 0]\n",
      "    dec train predicted > [3 3 8 3 4 2 7 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 7 9 3 2 4 5 3]\n",
      "    dec train predicted > [7 7 9 3 2 4 5 3 1]\n",
      "\n",
      "batch 49000\n",
      "  minibatch loss: 0.1251237541437149\n",
      "  sample 1:\n",
      "    enc input           > [6 5 7 9 4 4 7 0]\n",
      "    dec train predicted > [6 5 7 9 4 4 7 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [2 7 3 0 0 0 0 0]\n",
      "    dec train predicted > [2 7 3 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [8 9 7 7 2 6 0 0]\n",
      "    dec train predicted > [5 9 7 7 2 6 1 0 0]\n",
      "\n",
      "batch 50000\n",
      "  minibatch loss: 0.08682053536176682\n",
      "  sample 1:\n",
      "    enc input           > [4 2 7 2 0 0 0 0]\n",
      "    dec train predicted > [4 2 7 2 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [2 4 4 0 0 0 0 0]\n",
      "    dec train predicted > [2 4 4 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [3 5 2 4 0 0 0 0]\n",
      "    dec train predicted > [3 5 2 4 1 0 0 0 0]\n",
      "\n",
      "batch 51000\n",
      "  minibatch loss: 0.13243897259235382\n",
      "  sample 1:\n",
      "    enc input           > [7 5 5 0 0 0 0 0]\n",
      "    dec train predicted > [7 5 5 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [7 7 9 7 6 3 3 0]\n",
      "    dec train predicted > [7 7 9 7 6 3 3 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 9 2 0 0 0 0 0]\n",
      "    dec train predicted > [7 9 2 1 0 0 0 0 0]\n",
      "\n",
      "batch 52000\n",
      "  minibatch loss: 0.07681328058242798\n",
      "  sample 1:\n",
      "    enc input           > [2 9 5 2 5 8 3 0]\n",
      "    dec train predicted > [2 9 5 2 5 8 3 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 9 6 2 8 7 7 3]\n",
      "    dec train predicted > [3 9 6 2 8 7 7 3 1]\n",
      "  sample 3:\n",
      "    enc input           > [3 3 3 4 0 0 0 0]\n",
      "    dec train predicted > [3 3 3 4 1 0 0 0 0]\n",
      "\n",
      "batch 53000\n",
      "  minibatch loss: 0.07890034466981888\n",
      "  sample 1:\n",
      "    enc input           > [2 2 9 4 6 4 0 0]\n",
      "    dec train predicted > [2 2 9 4 6 4 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [5 8 6 9 8 7 8 3]\n",
      "    dec train predicted > [5 8 6 9 8 5 8 3 1]\n",
      "  sample 3:\n",
      "    enc input           > [9 6 7 7 6 2 0 0]\n",
      "    dec train predicted > [9 6 7 7 6 2 1 0 0]\n",
      "\n",
      "batch 54000\n",
      "  minibatch loss: 0.08589619398117065\n",
      "  sample 1:\n",
      "    enc input           > [6 2 5 3 7 0 0 0]\n",
      "    dec train predicted > [6 2 5 3 7 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [8 8 9 6 7 8 3 0]\n",
      "    dec train predicted > [8 8 9 6 7 8 3 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 6 6 7 9 3 0 0]\n",
      "    dec train predicted > [7 6 6 7 9 3 1 0 0]\n",
      "\n",
      "batch 55000\n",
      "  minibatch loss: 0.10034733265638351\n",
      "  sample 1:\n",
      "    enc input           > [4 7 2 8 0 0 0 0]\n",
      "    dec train predicted > [4 7 2 8 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [2 7 7 3 0 0 0 0]\n",
      "    dec train predicted > [4 7 7 3 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [6 7 4 3 9 0 0 0]\n",
      "    dec train predicted > [6 7 4 3 9 1 0 0 0]\n",
      "\n",
      "batch 56000\n",
      "  minibatch loss: 0.08506441861391068\n",
      "  sample 1:\n",
      "    enc input           > [3 4 4 9 8 4 8 0]\n",
      "    dec train predicted > [3 4 4 9 8 4 8 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [6 2 3 0 0 0 0 0]\n",
      "    dec train predicted > [6 2 3 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [6 2 6 6 0 0 0 0]\n",
      "    dec train predicted > [6 2 6 6 1 0 0 0 0]\n",
      "\n",
      "batch 57000\n",
      "  minibatch loss: 0.09202675521373749\n",
      "  sample 1:\n",
      "    enc input           > [3 3 5 5 5 6 0 0]\n",
      "    dec train predicted > [3 3 5 5 5 6 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 6 2 2 2 0 0 0]\n",
      "    dec train predicted > [3 6 2 2 2 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [9 4 9 4 8 9 8 7]\n",
      "    dec train predicted > [9 4 9 4 8 5 5 7 1]\n",
      "\n",
      "batch 58000\n",
      "  minibatch loss: 0.11747966706752777\n",
      "  sample 1:\n",
      "    enc input           > [6 9 5 6 3 8 3 5]\n",
      "    dec train predicted > [6 9 5 6 3 2 3 5 1]\n",
      "  sample 2:\n",
      "    enc input           > [3 3 6 6 8 0 0 0]\n",
      "    dec train predicted > [3 3 6 6 8 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [8 6 6 0 0 0 0 0]\n",
      "    dec train predicted > [8 6 6 1 0 0 0 0 0]\n",
      "\n",
      "batch 59000\n",
      "  minibatch loss: 0.08080718666315079\n",
      "  sample 1:\n",
      "    enc input           > [2 4 9 6 7 9 0 0]\n",
      "    dec train predicted > [2 4 9 6 7 9 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [5 2 8 6 2 6 6 7]\n",
      "    dec train predicted > [8 2 8 6 2 6 6 7 1]\n",
      "  sample 3:\n",
      "    enc input           > [4 4 4 9 4 0 0 0]\n",
      "    dec train predicted > [4 4 4 9 4 1 0 0 0]\n",
      "\n",
      "batch 60000\n",
      "  minibatch loss: 0.09612530469894409\n",
      "  sample 1:\n",
      "    enc input           > [2 7 5 2 2 0 0 0]\n",
      "    dec train predicted > [2 7 5 2 2 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [4 9 7 5 0 0 0 0]\n",
      "    dec train predicted > [4 9 7 5 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [6 8 4 0 0 0 0 0]\n",
      "    dec train predicted > [6 8 4 1 0 0 0 0 0]\n",
      "\n",
      "batch 61000\n",
      "  minibatch loss: 0.0795557051897049\n",
      "  sample 1:\n",
      "    enc input           > [3 2 5 0 0 0 0 0]\n",
      "    dec train predicted > [3 2 5 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [2 7 9 0 0 0 0 0]\n",
      "    dec train predicted > [2 7 9 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 5 7 4 0 0 0 0]\n",
      "    dec train predicted > [7 7 7 4 1 0 0 0 0]\n",
      "\n",
      "batch 62000\n",
      "  minibatch loss: 0.07965972274541855\n",
      "  sample 1:\n",
      "    enc input           > [6 6 7 6 9 3 8 0]\n",
      "    dec train predicted > [6 6 9 6 9 3 8 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [9 7 3 0 0 0 0 0]\n",
      "    dec train predicted > [9 7 3 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [3 3 5 0 0 0 0 0]\n",
      "    dec train predicted > [3 3 5 1 0 0 0 0 0]\n",
      "\n",
      "batch 63000\n",
      "  minibatch loss: 0.06053892895579338\n",
      "  sample 1:\n",
      "    enc input           > [7 7 6 0 0 0 0 0]\n",
      "    dec train predicted > [7 7 6 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [6 2 4 3 2 3 9 7]\n",
      "    dec train predicted > [6 2 4 3 2 3 9 7 1]\n",
      "  sample 3:\n",
      "    enc input           > [5 2 4 2 4 6 6 2]\n",
      "    dec train predicted > [5 2 4 2 4 6 6 2 1]\n",
      "\n",
      "batch 64000\n",
      "  minibatch loss: 0.06869596242904663\n",
      "  sample 1:\n",
      "    enc input           > [4 9 2 2 9 0 0 0]\n",
      "    dec train predicted > [4 9 2 2 9 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [5 3 4 3 6 0 0 0]\n",
      "    dec train predicted > [5 3 4 3 6 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [5 9 6 5 5 0 0 0]\n",
      "    dec train predicted > [5 9 6 5 5 1 0 0 0]\n",
      "\n",
      "batch 65000\n",
      "  minibatch loss: 0.10294130444526672\n",
      "  sample 1:\n",
      "    enc input           > [8 3 7 7 4 6 2 0]\n",
      "    dec train predicted > [5 3 7 7 4 6 2 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [4 3 2 7 6 0 0 0]\n",
      "    dec train predicted > [4 3 2 7 6 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [3 7 2 8 3 0 0 0]\n",
      "    dec train predicted > [3 7 2 8 3 1 0 0 0]\n",
      "\n",
      "batch 66000\n",
      "  minibatch loss: 0.11249476671218872\n",
      "  sample 1:\n",
      "    enc input           > [7 9 6 5 8 0 0 0]\n",
      "    dec train predicted > [7 9 6 5 8 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [5 3 6 0 0 0 0 0]\n",
      "    dec train predicted > [5 3 6 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [8 4 2 6 7 8 5 6]\n",
      "    dec train predicted > [8 4 2 6 7 8 6 6 1]\n",
      "\n",
      "batch 67000\n",
      "  minibatch loss: 0.058711059391498566\n",
      "  sample 1:\n",
      "    enc input           > [7 2 8 0 0 0 0 0]\n",
      "    dec train predicted > [7 2 8 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [7 6 3 8 5 4 6 2]\n",
      "    dec train predicted > [7 6 3 8 5 4 6 2 1]\n",
      "  sample 3:\n",
      "    enc input           > [9 3 2 2 6 6 8 0]\n",
      "    dec train predicted > [9 3 2 2 6 6 8 1 0]\n",
      "\n",
      "batch 68000\n",
      "  minibatch loss: 0.08393694460391998\n",
      "  sample 1:\n",
      "    enc input           > [7 9 3 2 4 0 0 0]\n",
      "    dec train predicted > [7 9 3 2 4 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [6 5 5 7 9 0 0 0]\n",
      "    dec train predicted > [6 5 5 7 9 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [6 6 2 2 9 4 7 0]\n",
      "    dec train predicted > [6 6 2 2 9 4 7 1 0]\n",
      "\n",
      "batch 69000\n",
      "  minibatch loss: 0.06295937299728394\n",
      "  sample 1:\n",
      "    enc input           > [2 3 5 0 0 0 0 0]\n",
      "    dec train predicted > [2 3 5 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [7 9 4 9 0 0 0 0]\n",
      "    dec train predicted > [7 9 4 9 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [4 2 4 9 8 0 0 0]\n",
      "    dec train predicted > [4 2 4 9 8 1 0 0 0]\n",
      "\n",
      "batch 70000\n",
      "  minibatch loss: 0.10624991357326508\n",
      "  sample 1:\n",
      "    enc input           > [3 2 2 5 6 7 0 0]\n",
      "    dec train predicted > [3 2 2 5 6 7 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 2 3 0 0 0 0 0]\n",
      "    dec train predicted > [3 2 3 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [3 4 3 4 7 0 0 0]\n",
      "    dec train predicted > [3 4 3 4 7 1 0 0 0]\n",
      "\n",
      "batch 71000\n",
      "  minibatch loss: 0.06760768592357635\n",
      "  sample 1:\n",
      "    enc input           > [8 6 5 0 0 0 0 0]\n",
      "    dec train predicted > [8 6 5 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [8 4 4 2 0 0 0 0]\n",
      "    dec train predicted > [2 4 4 2 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [4 3 8 3 9 6 0 0]\n",
      "    dec train predicted > [4 3 8 3 9 6 1 0 0]\n",
      "\n",
      "batch 72000\n",
      "  minibatch loss: 0.12059029191732407\n",
      "  sample 1:\n",
      "    enc input           > [8 8 6 6 5 9 3 0]\n",
      "    dec train predicted > [8 8 6 6 9 9 3 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [5 2 3 9 7 4 5 3]\n",
      "    dec train predicted > [5 2 3 9 7 4 7 3 1]\n",
      "  sample 3:\n",
      "    enc input           > [3 6 3 4 7 9 6 6]\n",
      "    dec train predicted > [3 6 3 4 7 3 6 6 1]\n",
      "\n",
      "batch 73000\n",
      "  minibatch loss: 0.09205712378025055\n",
      "  sample 1:\n",
      "    enc input           > [4 3 8 3 9 3 0 0]\n",
      "    dec train predicted > [4 3 8 3 9 3 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [8 5 7 5 8 4 0 0]\n",
      "    dec train predicted > [8 5 7 5 8 4 1 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [2 5 6 2 0 0 0 0]\n",
      "    dec train predicted > [2 5 6 2 1 0 0 0 0]\n",
      "\n",
      "batch 74000\n",
      "  minibatch loss: 0.06519684940576553\n",
      "  sample 1:\n",
      "    enc input           > [7 5 8 4 2 5 5 0]\n",
      "    dec train predicted > [7 5 8 4 2 5 5 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [9 3 3 2 4 0 0 0]\n",
      "    dec train predicted > [9 3 3 2 4 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [3 8 6 4 0 0 0 0]\n",
      "    dec train predicted > [3 8 6 4 1 0 0 0 0]\n",
      "\n",
      "batch 75000\n",
      "  minibatch loss: 0.06833715736865997\n",
      "  sample 1:\n",
      "    enc input           > [8 4 9 2 5 2 0 0]\n",
      "    dec train predicted > [8 4 9 2 5 2 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [6 6 7 4 3 7 8 0]\n",
      "    dec train predicted > [6 6 7 4 3 7 8 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [9 7 6 9 6 3 5 4]\n",
      "    dec train predicted > [9 9 6 9 6 4 5 4 1]\n",
      "\n",
      "batch 76000\n",
      "  minibatch loss: 0.05804649740457535\n",
      "  sample 1:\n",
      "    enc input           > [7 3 5 7 9 0 0 0]\n",
      "    dec train predicted > [7 3 5 7 9 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [4 7 7 0 0 0 0 0]\n",
      "    dec train predicted > [4 7 7 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [9 3 7 8 7 3 6 0]\n",
      "    dec train predicted > [9 3 7 5 7 3 6 1 0]\n",
      "\n",
      "batch 77000\n",
      "  minibatch loss: 0.07503343373537064\n",
      "  sample 1:\n",
      "    enc input           > [6 4 2 7 9 7 6 0]\n",
      "    dec train predicted > [6 4 2 7 9 7 6 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [6 8 8 4 5 6 0 0]\n",
      "    dec train predicted > [6 8 8 4 5 6 1 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [5 4 9 4 8 5 2 8]\n",
      "    dec train predicted > [5 4 9 4 8 5 2 8 1]\n",
      "\n",
      "batch 78000\n",
      "  minibatch loss: 0.1484459638595581\n",
      "  sample 1:\n",
      "    enc input           > [3 8 8 7 6 0 0 0]\n",
      "    dec train predicted > [3 8 8 7 6 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [5 8 2 5 6 7 4 0]\n",
      "    dec train predicted > [5 8 2 5 6 7 4 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 3 6 0 0 0 0 0]\n",
      "    dec train predicted > [7 3 6 1 0 0 0 0 0]\n",
      "\n",
      "batch 79000\n",
      "  minibatch loss: 0.058953750878572464\n",
      "  sample 1:\n",
      "    enc input           > [3 3 5 5 8 5 3 0]\n",
      "    dec train predicted > [3 3 5 5 8 5 3 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [7 5 5 5 0 0 0 0]\n",
      "    dec train predicted > [7 5 5 5 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [8 5 3 8 5 0 0 0]\n",
      "    dec train predicted > [8 5 3 8 7 1 0 0 0]\n",
      "\n",
      "batch 80000\n",
      "  minibatch loss: 0.06799878180027008\n",
      "  sample 1:\n",
      "    enc input           > [7 3 3 7 4 0 0 0]\n",
      "    dec train predicted > [7 3 3 7 4 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [4 4 5 6 4 9 8 6]\n",
      "    dec train predicted > [4 4 5 3 4 9 8 6 1]\n",
      "  sample 3:\n",
      "    enc input           > [2 3 5 3 0 0 0 0]\n",
      "    dec train predicted > [2 3 5 3 1 0 0 0 0]\n",
      "\n",
      "batch 81000\n",
      "  minibatch loss: 0.08230670541524887\n",
      "  sample 1:\n",
      "    enc input           > [9 3 5 3 5 5 4 0]\n",
      "    dec train predicted > [9 3 5 3 5 5 4 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 4 9 2 7 0 0 0]\n",
      "    dec train predicted > [3 4 9 2 7 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [5 3 6 2 5 2 4 7]\n",
      "    dec train predicted > [5 3 6 2 8 2 4 7 1]\n",
      "\n",
      "batch 82000\n",
      "  minibatch loss: 0.06821895390748978\n",
      "  sample 1:\n",
      "    enc input           > [2 5 6 9 0 0 0 0]\n",
      "    dec train predicted > [2 5 6 9 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [7 5 2 3 4 8 7 8]\n",
      "    dec train predicted > [7 5 2 3 4 5 7 8 1]\n",
      "  sample 3:\n",
      "    enc input           > [5 3 2 4 0 0 0 0]\n",
      "    dec train predicted > [5 3 2 4 1 0 0 0 0]\n",
      "\n",
      "batch 83000\n",
      "  minibatch loss: 0.06609655916690826\n",
      "  sample 1:\n",
      "    enc input           > [7 2 8 4 2 5 8 3]\n",
      "    dec train predicted > [7 2 8 4 2 5 6 3 1]\n",
      "  sample 2:\n",
      "    enc input           > [3 5 9 0 0 0 0 0]\n",
      "    dec train predicted > [3 5 9 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [2 3 4 9 6 0 0 0]\n",
      "    dec train predicted > [2 3 4 9 6 1 0 0 0]\n",
      "\n",
      "batch 84000\n",
      "  minibatch loss: 0.059434372931718826\n",
      "  sample 1:\n",
      "    enc input           > [8 5 4 9 5 3 8 2]\n",
      "    dec train predicted > [8 5 4 9 5 3 8 2 1]\n",
      "  sample 2:\n",
      "    enc input           > [8 8 2 6 9 0 0 0]\n",
      "    dec train predicted > [8 8 2 6 9 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [5 4 8 0 0 0 0 0]\n",
      "    dec train predicted > [5 4 8 1 0 0 0 0 0]\n",
      "\n",
      "batch 85000\n",
      "  minibatch loss: 0.04226361960172653\n",
      "  sample 1:\n",
      "    enc input           > [7 7 2 5 7 6 0 0]\n",
      "    dec train predicted > [7 7 2 5 7 6 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [8 4 6 3 0 0 0 0]\n",
      "    dec train predicted > [8 4 6 3 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [9 3 5 5 7 3 0 0]\n",
      "    dec train predicted > [9 3 5 5 7 3 1 0 0]\n",
      "\n",
      "batch 86000\n",
      "  minibatch loss: 0.05533894523978233\n",
      "  sample 1:\n",
      "    enc input           > [5 2 6 7 5 0 0 0]\n",
      "    dec train predicted > [5 2 6 7 5 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [6 2 8 2 6 0 0 0]\n",
      "    dec train predicted > [6 2 8 2 6 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [6 9 6 3 9 0 0 0]\n",
      "    dec train predicted > [6 9 6 3 9 1 0 0 0]\n",
      "\n",
      "batch 87000\n",
      "  minibatch loss: 0.05768728628754616\n",
      "  sample 1:\n",
      "    enc input           > [9 9 8 9 7 8 4 2]\n",
      "    dec train predicted > [9 9 8 9 7 8 4 2 1]\n",
      "  sample 2:\n",
      "    enc input           > [2 6 6 3 5 9 0 0]\n",
      "    dec train predicted > [2 6 6 3 5 9 1 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [2 5 9 7 2 5 2 6]\n",
      "    dec train predicted > [2 5 9 7 2 5 2 6 1]\n",
      "\n",
      "batch 88000\n",
      "  minibatch loss: 0.050071630626916885\n",
      "  sample 1:\n",
      "    enc input           > [6 9 3 0 0 0 0 0]\n",
      "    dec train predicted > [6 9 3 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [9 9 7 7 6 0 0 0]\n",
      "    dec train predicted > [9 9 7 7 6 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [8 2 4 9 4 0 0 0]\n",
      "    dec train predicted > [8 2 4 9 4 1 0 0 0]\n",
      "\n",
      "batch 89000\n",
      "  minibatch loss: 0.04040560871362686\n",
      "  sample 1:\n",
      "    enc input           > [5 6 2 7 8 3 2 0]\n",
      "    dec train predicted > [5 6 2 7 8 3 2 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [7 2 4 9 0 0 0 0]\n",
      "    dec train predicted > [7 2 4 9 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [3 9 9 7 9 7 4 0]\n",
      "    dec train predicted > [3 9 9 7 9 7 4 1 0]\n",
      "\n",
      "batch 90000\n",
      "  minibatch loss: 0.055022384971380234\n",
      "  sample 1:\n",
      "    enc input           > [6 4 2 9 8 0 0 0]\n",
      "    dec train predicted > [6 4 2 9 8 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [5 3 8 3 2 5 0 0]\n",
      "    dec train predicted > [5 3 8 3 2 5 1 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [8 9 9 9 2 0 0 0]\n",
      "    dec train predicted > [8 9 9 9 2 1 0 0 0]\n",
      "\n",
      "batch 91000\n",
      "  minibatch loss: 0.07805629819631577\n",
      "  sample 1:\n",
      "    enc input           > [6 5 2 7 3 9 7 0]\n",
      "    dec train predicted > [6 5 2 7 3 9 7 1 0]\n",
      "  sample 2:\n",
      "    enc input           > [8 5 7 4 2 2 0 0]\n",
      "    dec train predicted > [8 5 7 4 2 2 1 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [3 7 7 5 4 6 7 5]\n",
      "    dec train predicted > [3 7 7 5 4 6 7 5 1]\n",
      "\n",
      "batch 92000\n",
      "  minibatch loss: 0.055099643766880035\n",
      "  sample 1:\n",
      "    enc input           > [9 2 9 4 3 7 4 4]\n",
      "    dec train predicted > [9 2 9 4 3 7 4 4 1]\n",
      "  sample 2:\n",
      "    enc input           > [8 6 5 9 3 2 4 0]\n",
      "    dec train predicted > [8 6 5 9 3 2 4 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [3 6 6 9 5 2 8 7]\n",
      "    dec train predicted > [3 6 6 9 5 2 8 7 1]\n",
      "\n",
      "batch 93000\n",
      "  minibatch loss: 0.04349450394511223\n",
      "  sample 1:\n",
      "    enc input           > [6 7 4 0 0 0 0 0]\n",
      "    dec train predicted > [6 7 4 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 8 7 0 0 0 0 0]\n",
      "    dec train predicted > [3 8 7 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [6 8 7 7 2 0 0 0]\n",
      "    dec train predicted > [6 8 7 7 2 1 0 0 0]\n",
      "\n",
      "batch 94000\n",
      "  minibatch loss: 0.03787858784198761\n",
      "  sample 1:\n",
      "    enc input           > [7 5 3 8 8 2 0 0]\n",
      "    dec train predicted > [7 5 3 8 8 2 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [9 7 3 0 0 0 0 0]\n",
      "    dec train predicted > [9 7 3 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [9 7 2 9 3 0 0 0]\n",
      "    dec train predicted > [9 7 2 9 3 1 0 0 0]\n",
      "\n",
      "batch 95000\n",
      "  minibatch loss: 0.06564287096261978\n",
      "  sample 1:\n",
      "    enc input           > [4 3 6 4 0 0 0 0]\n",
      "    dec train predicted > [4 3 6 4 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [7 2 4 3 2 0 0 0]\n",
      "    dec train predicted > [7 2 4 3 2 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [9 5 4 0 0 0 0 0]\n",
      "    dec train predicted > [9 5 4 1 0 0 0 0 0]\n",
      "\n",
      "batch 96000\n",
      "  minibatch loss: 0.07709616422653198\n",
      "  sample 1:\n",
      "    enc input           > [4 4 2 4 7 9 3 5]\n",
      "    dec train predicted > [4 4 2 4 7 9 9 5 1]\n",
      "  sample 2:\n",
      "    enc input           > [7 8 2 8 8 7 9 6]\n",
      "    dec train predicted > [5 8 8 8 5 7 9 6 1]\n",
      "  sample 3:\n",
      "    enc input           > [8 3 6 5 0 0 0 0]\n",
      "    dec train predicted > [8 3 6 5 1 0 0 0 0]\n",
      "\n",
      "batch 97000\n",
      "  minibatch loss: 0.12001687288284302\n",
      "  sample 1:\n",
      "    enc input           > [3 8 2 9 9 6 6 3]\n",
      "    dec train predicted > [3 8 6 9 9 6 6 6 1]\n",
      "  sample 2:\n",
      "    enc input           > [4 6 3 8 6 8 8 0]\n",
      "    dec train predicted > [4 6 3 8 6 8 8 1 0]\n",
      "  sample 3:\n",
      "    enc input           > [6 8 8 6 0 0 0 0]\n",
      "    dec train predicted > [6 8 8 6 1 0 0 0 0]\n",
      "\n",
      "batch 98000\n",
      "  minibatch loss: 0.07364653795957565\n",
      "  sample 1:\n",
      "    enc input           > [8 3 9 0 0 0 0 0]\n",
      "    dec train predicted > [8 3 9 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 9 9 8 3 9 0 0]\n",
      "    dec train predicted > [3 9 9 8 3 9 1 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [5 6 6 8 3 3 6 2]\n",
      "    dec train predicted > [5 6 6 6 3 3 6 6 1]\n",
      "\n",
      "batch 99000\n",
      "  minibatch loss: 0.044901177287101746\n",
      "  sample 1:\n",
      "    enc input           > [5 4 3 8 5 0 0 0]\n",
      "    dec train predicted > [5 4 3 8 5 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 9 6 6 3 7 6 2]\n",
      "    dec train predicted > [3 9 6 6 3 7 6 2 1]\n",
      "  sample 3:\n",
      "    enc input           > [9 5 5 8 9 9 0 0]\n",
      "    dec train predicted > [9 5 5 8 9 9 1 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 100000\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "def transpose(l):\n",
    "    return [x.T for x in l]\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            for i, (e_in, d_tg, dt_in, dt_tg, dt_pred) in enumerate(zip(\n",
    "                    fd[encoder_inputs].T, \n",
    "                    fd[decoder_targets].T,\n",
    "                    *transpose(sess.run([\n",
    "                        decoder_train_inputs,\n",
    "                        decoder_train_targets,\n",
    "                        decoder_prediction_train,\n",
    "                    ], fd))\n",
    "                )):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    enc input           > {}'.format(e_in))\n",
    "                #print('    dec target          > {}'.format(d_tg))\n",
    "                #print('    dec train input     > {}'.format(dt_in))\n",
    "                #print('    dec train target    > {}'.format(dt_tg))\n",
    "                print('    dec train predicted > {}'.format(dt_pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1972 after 10177100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAFkCAYAAABmeZIKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYkOgAQAAYkOgAQAAYkOgAQAAYvP/Ad/qpfgsTwTdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bfa7208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'\\\n",
    "      .format(loss_track[-1], len(loss_track)*batch_size,\n",
    "              batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow attention slows down convergence significantly and makes it volatile."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
