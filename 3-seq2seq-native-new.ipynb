{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "# Playing with new 2017 `tf.contrib.seq2seq`\n",
    "\n",
    "I just discovered that a [new dynamic seq2seq implementation](https://github.com/tensorflow/tensorflow/tree/24466c2e6d32621cd85f0a78d47df6eed2c5c5a6/tensorflow/contrib/seq2seq) was recently merged into master. Naturally, I wanted to try it out.\n",
    "\n",
    "`Working with commit 24466c2e6d32621cd85f0a78d47df6eed2c5c5a6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "outputExpanded": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0-alpha'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is time-major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "decoder_targets_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='decoder_targets_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, `decoder_targets` would serve as basis for both `decoder_inputs` and decoder logits. This means that their shapes should be compatible.\n",
    "\n",
    "Here we do a bit of plumbing to set this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_size, batch_size = tf.unstack(tf.shape(decoder_targets))\n",
    "\n",
    "EOS_SLICE = tf.ones([1, batch_size], dtype=tf.int32)\n",
    "PAD_SLICE = tf.zeros([1, batch_size], dtype=tf.int32)\n",
    "\n",
    "decoder_train_inputs = tf.concat_v2([EOS_SLICE, decoder_targets], axis=0)\n",
    "decoder_train_length = decoder_targets_length + 1\n",
    "\n",
    "decoder_train_targets = tf.concat_v2([decoder_targets, PAD_SLICE], axis=0)\n",
    "decoder_train_targets_seqlen, _ = tf.unstack(tf.shape(decoder_train_targets))\n",
    "decoder_train_targets_eos_mask = tf.transpose(tf.one_hot(decoder_train_length - 1, decoder_train_targets_seqlen, dtype=tf.int32), [1, 0])\n",
    "decoder_train_targets = tf.add(\n",
    "    decoder_train_targets,\n",
    "    decoder_train_targets_eos_mask,\n",
    ")  # hacky way to put EOS symbol at the end of target sequence\n",
    "\n",
    "loss_weights = tf.ones([batch_size, tf.reduce_max(decoder_train_length)], dtype=tf.float32, name=\"loss_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# oh=tf.one_hot(decoder_train_length, decoder_train_targets_seqlen, dtype=tf.int32)\n",
    "\n",
    "# sess.run([oh, decoder_train_targets_eos_mask], fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import (LSTMCell, LSTMStateTuple, EmbeddingWrapper)\n",
    "import tensorflow.contrib.seq2seq as seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.contrib.layers import embedding_lookup_unique\n",
    "\n",
    "def embedding(inputs, embedding_classes, embedding_size, scope=None):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "\n",
    "        # Uniform(-sqrt(3), sqrt(3)) has variance=1.\n",
    "        sqrt3 = math.sqrt(3)\n",
    "        initializer = tf.random_uniform_initializer(-sqrt3, sqrt3)\n",
    "        \n",
    "        embedding_values = tf.get_variable(\n",
    "            name=\"embedding\",\n",
    "            shape=[embedding_classes, embedding_size],\n",
    "            initializer=initializer,\n",
    "            dtype=tf.float32)\n",
    "        \n",
    "        return embedding_lookup_unique(embedding_values, inputs)\n",
    "\n",
    "with tf.variable_scope(\"embedding\") as scope:\n",
    "    encoder_inputs_embedded = embedding(encoder_inputs,\n",
    "                                        vocab_size,\n",
    "                                        input_embedding_size,\n",
    "                                        scope)\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "\n",
    "    decoder_train_inputs_embedded = embedding(decoder_train_inputs,\n",
    "                                              vocab_size,\n",
    "                                              input_embedding_size,\n",
    "                                              scope)\n",
    "    \n",
    "    # we'll need matrix for inference\n",
    "    embedding_matrix = tf.get_variable(\"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\") as scope:\n",
    "    encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "    \n",
    "    ((encoder_fw_outputs,\n",
    "      encoder_bw_outputs),\n",
    "     (encoder_fw_state,\n",
    "      encoder_bw_state)) = (\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                        cell_bw=encoder_cell,\n",
    "                                        inputs=encoder_inputs_embedded,\n",
    "                                        sequence_length=encoder_inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True, scope=scope)\n",
    "        )\n",
    "\n",
    "    encoder_outputs = tf.concat_v2((encoder_fw_outputs, encoder_fw_outputs), 2)\n",
    "\n",
    "    encoder_state_c = tf.concat_v2(\n",
    "        (encoder_fw_state.c, encoder_bw_state.c), 1)\n",
    "\n",
    "    encoder_state_h = tf.concat_v2(\n",
    "        (encoder_fw_state.h, encoder_bw_state.h), 1)\n",
    "\n",
    "    encoder_state = LSTMStateTuple(\n",
    "        c=encoder_state_c,\n",
    "        h=encoder_state_h\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "\n",
    "with tf.variable_scope(\"Decoder\") as scope:\n",
    "    decoder_fn_train = seq2seq.simple_decoder_fn_train(encoder_state=encoder_state)\n",
    "    \n",
    "    (\n",
    "        decoder_outputs_train,\n",
    "        decoder_state_train,\n",
    "        decoder_context_state_train\n",
    "    ) = (\n",
    "        seq2seq.dynamic_rnn_decoder(\n",
    "           cell=decoder_cell,\n",
    "           decoder_fn=decoder_fn_train,\n",
    "           inputs=decoder_train_inputs_embedded,\n",
    "           sequence_length=decoder_train_length,\n",
    "           time_major=True,\n",
    "           scope=scope,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    def output_fn(outputs):\n",
    "        return tf.contrib.layers.linear(outputs, vocab_size, scope=scope)\n",
    "    \n",
    "    decoder_logits_train = output_fn(decoder_outputs_train)\n",
    "    \n",
    "    decoder_prediction_train = tf.argmax(decoder_logits_train, axis=-1, name='decoder_prediction')\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    \n",
    "    decoder_fn_inference = seq2seq.simple_decoder_fn_inference(\n",
    "        output_fn=output_fn,\n",
    "        encoder_state=encoder_state,\n",
    "        embeddings=embedding_matrix,\n",
    "        start_of_sequence_id=1,\n",
    "        end_of_sequence_id=1,\n",
    "        maximum_length=tf.reduce_max(encoder_inputs_length) + 3,\n",
    "        num_decoder_symbols=vocab_size,\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        decoder_outputs_inference,\n",
    "        decoder_state_inference,\n",
    "        decoder_context_state_inference\n",
    "    ) = (\n",
    "        seq2seq.dynamic_rnn_decoder(\n",
    "           cell=decoder_cell,\n",
    "           decoder_fn=decoder_fn_inference,\n",
    "           time_major=True,\n",
    "           scope=scope,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    ## @TODO: in case of inference decoder_outputs_inference is logits, not cell_output?\n",
    "\n",
    "\n",
    "seqloss = seq2seq.sequence_loss(\n",
    "    logits=tf.transpose(decoder_logits_train, [1, 0, 2]),\n",
    "    targets=tf.transpose(decoder_train_targets, [1, 0]),\n",
    "    weights=loss_weights)\n",
    "\n",
    "loss = tf.reduce_mean(seqloss)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 6, 8, 6]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "seq = next(batches)[0]\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    inputs_, inputs_length_ = helpers.batch(batch)\n",
    "    return {\n",
    "        encoder_inputs: inputs_,\n",
    "        encoder_inputs_length: inputs_length_,\n",
    "        decoder_targets: inputs_,\n",
    "        decoder_targets_length: inputs_length_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"encoder_inputs:0\", shape=(?, ?), dtype=int32) 8\n",
      "Tensor(\"encoder_inputs_length:0\", shape=(?,), dtype=int32) 100\n",
      "Tensor(\"decoder_targets_length:0\", shape=(?,), dtype=int32) 100\n",
      "Tensor(\"decoder_targets:0\", shape=(?, ?), dtype=int32) 8\n"
     ]
    }
   ],
   "source": [
    "for k, v in next_feed().items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.307854175567627\n",
      "  sample 1:\n",
      "    enc input           > [9 8 8 3 0 0 0 0]\n",
      "    dec target          > [9 8 8 3 0 0 0 0]\n",
      "    dec train input     > [1 9 8 8 3 0 0 0 0]\n",
      "    dec train target    > [9 8 8 3 1 0 0 0 0]\n",
      "    dec train predicted > [0 2 8 5 5 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [9 4 9 9 6 0 0 0]\n",
      "    dec target          > [9 4 9 9 6 0 0 0]\n",
      "    dec train input     > [1 9 4 9 9 6 0 0 0]\n",
      "    dec train target    > [9 4 9 9 6 1 0 0 0]\n",
      "    dec train predicted > [2 2 3 3 3 3 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [2 3 6 8 3 0 0 0]\n",
      "    dec target          > [2 3 6 8 3 0 0 0]\n",
      "    dec train input     > [1 2 3 6 8 3 0 0 0]\n",
      "    dec train target    > [2 3 6 8 3 1 0 0 0]\n",
      "    dec train predicted > [2 2 0 0 5 1 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.35761183500289917\n",
      "  sample 1:\n",
      "    enc input           > [8 4 6 8 6 6 7 9]\n",
      "    dec target          > [8 4 6 8 6 6 7 9]\n",
      "    dec train input     > [1 8 4 6 8 6 6 7 9]\n",
      "    dec train target    > [8 4 6 8 6 6 7 9 1]\n",
      "    dec train predicted > [8 6 6 6 6 9 9 9 1]\n",
      "  sample 2:\n",
      "    enc input           > [9 4 6 6 6 0 0 0]\n",
      "    dec target          > [9 4 6 6 6 0 0 0]\n",
      "    dec train input     > [1 9 4 6 6 6 0 0 0]\n",
      "    dec train target    > [9 4 6 6 6 1 0 0 0]\n",
      "    dec train predicted > [9 6 6 6 6 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 6 7 5 6 8 8 7]\n",
      "    dec target          > [7 6 7 5 6 8 8 7]\n",
      "    dec train input     > [1 7 6 7 5 6 8 8 7]\n",
      "    dec train target    > [7 6 7 5 6 8 8 7 1]\n",
      "    dec train predicted > [7 6 7 5 8 8 8 7 1]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.13941600918769836\n",
      "  sample 1:\n",
      "    enc input           > [8 2 8 0 0 0 0 0]\n",
      "    dec target          > [8 2 8 0 0 0 0 0]\n",
      "    dec train input     > [1 8 2 8 0 0 0 0 0]\n",
      "    dec train target    > [8 2 8 1 0 0 0 0 0]\n",
      "    dec train predicted > [8 2 8 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 5 4 0 0 0 0 0]\n",
      "    dec target          > [3 5 4 0 0 0 0 0]\n",
      "    dec train input     > [1 3 5 4 0 0 0 0 0]\n",
      "    dec train target    > [3 5 4 1 0 0 0 0 0]\n",
      "    dec train predicted > [3 5 4 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [2 5 2 0 0 0 0 0]\n",
      "    dec target          > [2 5 2 0 0 0 0 0]\n",
      "    dec train input     > [1 2 5 2 0 0 0 0 0]\n",
      "    dec train target    > [2 5 2 1 0 0 0 0 0]\n",
      "    dec train predicted > [2 5 2 1 0 0 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.06648726761341095\n",
      "  sample 1:\n",
      "    enc input           > [7 3 7 0 0 0 0 0]\n",
      "    dec target          > [7 3 7 0 0 0 0 0]\n",
      "    dec train input     > [1 7 3 7 0 0 0 0 0]\n",
      "    dec train target    > [7 3 7 1 0 0 0 0 0]\n",
      "    dec train predicted > [7 3 7 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [6 4 3 4 4 0 0 0]\n",
      "    dec target          > [6 4 3 4 4 0 0 0]\n",
      "    dec train input     > [1 6 4 3 4 4 0 0 0]\n",
      "    dec train target    > [6 4 3 4 4 1 0 0 0]\n",
      "    dec train predicted > [6 4 3 4 4 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [4 3 4 8 3 5 6 0]\n",
      "    dec target          > [4 3 4 8 3 5 6 0]\n",
      "    dec train input     > [1 4 3 4 8 3 5 6 0]\n",
      "    dec train target    > [4 3 4 8 3 5 6 1 0]\n",
      "    dec train predicted > [4 3 4 8 3 5 6 1 0]\n",
      "\n",
      "batch 4000\n",
      "  minibatch loss: 0.04015330970287323\n",
      "  sample 1:\n",
      "    enc input           > [7 2 3 7 0 0 0 0]\n",
      "    dec target          > [7 2 3 7 0 0 0 0]\n",
      "    dec train input     > [1 7 2 3 7 0 0 0 0]\n",
      "    dec train target    > [7 2 3 7 1 0 0 0 0]\n",
      "    dec train predicted > [7 2 3 7 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 6 4 8 0 0 0 0]\n",
      "    dec target          > [3 6 4 8 0 0 0 0]\n",
      "    dec train input     > [1 3 6 4 8 0 0 0 0]\n",
      "    dec train target    > [3 6 4 8 1 0 0 0 0]\n",
      "    dec train predicted > [3 6 4 8 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [2 2 4 6 7 0 0 0]\n",
      "    dec target          > [2 2 4 6 7 0 0 0]\n",
      "    dec train input     > [1 2 2 4 6 7 0 0 0]\n",
      "    dec train target    > [2 2 4 6 7 1 0 0 0]\n",
      "    dec train predicted > [2 2 4 6 7 1 0 0 0]\n",
      "\n",
      "batch 5000\n",
      "  minibatch loss: 0.025384727865457535\n",
      "  sample 1:\n",
      "    enc input           > [6 8 2 6 4 0 0 0]\n",
      "    dec target          > [6 8 2 6 4 0 0 0]\n",
      "    dec train input     > [1 6 8 2 6 4 0 0 0]\n",
      "    dec train target    > [6 8 2 6 4 1 0 0 0]\n",
      "    dec train predicted > [6 8 2 6 4 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [3 3 7 3 9 5 8 2]\n",
      "    dec target          > [3 3 7 3 9 5 8 2]\n",
      "    dec train input     > [1 3 3 7 3 9 5 8 2]\n",
      "    dec train target    > [3 3 7 3 9 5 8 2 1]\n",
      "    dec train predicted > [3 3 7 3 9 5 8 2 1]\n",
      "  sample 3:\n",
      "    enc input           > [2 7 2 9 3 6 0 0]\n",
      "    dec target          > [2 7 2 9 3 6 0 0]\n",
      "    dec train input     > [1 2 7 2 9 3 6 0 0]\n",
      "    dec train target    > [2 7 2 9 3 6 1 0 0]\n",
      "    dec train predicted > [2 7 2 9 3 6 1 0 0]\n",
      "\n",
      "batch 6000\n",
      "  minibatch loss: 0.01773851364850998\n",
      "  sample 1:\n",
      "    enc input           > [6 9 9 7 0 0 0 0]\n",
      "    dec target          > [6 9 9 7 0 0 0 0]\n",
      "    dec train input     > [1 6 9 9 7 0 0 0 0]\n",
      "    dec train target    > [6 9 9 7 1 0 0 0 0]\n",
      "    dec train predicted > [6 9 9 7 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [2 6 4 5 3 0 0 0]\n",
      "    dec target          > [2 6 4 5 3 0 0 0]\n",
      "    dec train input     > [1 2 6 4 5 3 0 0 0]\n",
      "    dec train target    > [2 6 4 5 3 1 0 0 0]\n",
      "    dec train predicted > [2 6 4 5 3 1 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [6 2 2 3 2 2 7 3]\n",
      "    dec target          > [6 2 2 3 2 2 7 3]\n",
      "    dec train input     > [1 6 2 2 3 2 2 7 3]\n",
      "    dec train target    > [6 2 2 3 2 2 7 3 1]\n",
      "    dec train predicted > [6 2 2 3 2 2 7 3 1]\n",
      "\n",
      "batch 7000\n",
      "  minibatch loss: 0.01024711038917303\n",
      "  sample 1:\n",
      "    enc input           > [2 8 2 6 9 0 0 0]\n",
      "    dec target          > [2 8 2 6 9 0 0 0]\n",
      "    dec train input     > [1 2 8 2 6 9 0 0 0]\n",
      "    dec train target    > [2 8 2 6 9 1 0 0 0]\n",
      "    dec train predicted > [2 8 2 6 9 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [5 4 7 9 0 0 0 0]\n",
      "    dec target          > [5 4 7 9 0 0 0 0]\n",
      "    dec train input     > [1 5 4 7 9 0 0 0 0]\n",
      "    dec train target    > [5 4 7 9 1 0 0 0 0]\n",
      "    dec train predicted > [5 4 7 9 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    enc input           > [7 9 7 2 2 0 0 0]\n",
      "    dec target          > [7 9 7 2 2 0 0 0]\n",
      "    dec train input     > [1 7 9 7 2 2 0 0 0]\n",
      "    dec train target    > [7 9 7 2 2 1 0 0 0]\n",
      "    dec train predicted > [7 9 7 2 2 1 0 0 0]\n",
      "\n",
      "batch 8000\n",
      "  minibatch loss: 0.008820915594696999\n",
      "  sample 1:\n",
      "    enc input           > [9 8 6 7 3 5 0 0]\n",
      "    dec target          > [9 8 6 7 3 5 0 0]\n",
      "    dec train input     > [1 9 8 6 7 3 5 0 0]\n",
      "    dec train target    > [9 8 6 7 3 5 1 0 0]\n",
      "    dec train predicted > [9 8 6 7 3 5 1 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [5 7 9 4 4 4 6 5]\n",
      "    dec target          > [5 7 9 4 4 4 6 5]\n",
      "    dec train input     > [1 5 7 9 4 4 4 6 5]\n",
      "    dec train target    > [5 7 9 4 4 4 6 5 1]\n",
      "    dec train predicted > [5 7 9 4 4 4 6 5 1]\n",
      "  sample 3:\n",
      "    enc input           > [4 6 6 9 2 7 9 6]\n",
      "    dec target          > [4 6 6 9 2 7 9 6]\n",
      "    dec train input     > [1 4 6 6 9 2 7 9 6]\n",
      "    dec train target    > [4 6 6 9 2 7 9 6 1]\n",
      "    dec train predicted > [4 6 6 9 2 9 9 6 1]\n",
      "\n",
      "batch 9000\n",
      "  minibatch loss: 0.009653369896113873\n",
      "  sample 1:\n",
      "    enc input           > [3 2 8 7 3 0 0 0]\n",
      "    dec target          > [3 2 8 7 3 0 0 0]\n",
      "    dec train input     > [1 3 2 8 7 3 0 0 0]\n",
      "    dec train target    > [3 2 8 7 3 1 0 0 0]\n",
      "    dec train predicted > [3 2 8 7 3 1 0 0 0]\n",
      "  sample 2:\n",
      "    enc input           > [8 8 4 2 4 4 9 3]\n",
      "    dec target          > [8 8 4 2 4 4 9 3]\n",
      "    dec train input     > [1 8 8 4 2 4 4 9 3]\n",
      "    dec train target    > [8 8 4 2 4 4 9 3 1]\n",
      "    dec train predicted > [8 8 4 2 4 4 9 3 1]\n",
      "  sample 3:\n",
      "    enc input           > [3 9 8 2 0 0 0 0]\n",
      "    dec target          > [3 9 8 2 0 0 0 0]\n",
      "    dec train input     > [1 3 9 8 2 0 0 0 0]\n",
      "    dec train target    > [3 9 8 2 1 0 0 0 0]\n",
      "    dec train predicted > [3 9 8 2 1 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 10000\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "def transpose(l):\n",
    "    return [x.T for x in l]\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            for i, (e_in, d_tg, dt_in, dt_tg, dt_pred) in enumerate(zip(\n",
    "                    fd[encoder_inputs].T, \n",
    "                    fd[decoder_targets].T,\n",
    "                    *transpose(sess.run([\n",
    "                        decoder_train_inputs,\n",
    "                        decoder_train_targets,\n",
    "                        decoder_prediction_train,\n",
    "                    ], fd))\n",
    "                )):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    enc input           > {}'.format(e_in))\n",
    "                print('    dec target          > {}'.format(d_tg))\n",
    "                print('    dec train input     > {}'.format(dt_in))\n",
    "                print('    dec train target    > {}'.format(dt_tg))\n",
    "                print('    dec train predicted > {}'.format(dt_pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0071 after 1000000 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XmYXFWd//H3NwtJICSILBFBlhEQUJYEBsIiCAIjGhQV\ntEWUcQEHmdEgI4oy446CyKa4K3vcfuAgMoCoMIoEhu4AGiAwLCIQwhYTIlm7z++PU21X71Xd1XWr\nut+v56mn6t57bt1vnXSqP33uFiklJEmSamFc0QVIkqTRw2AhSZJqxmAhSZJqxmAhSZJqxmAhSZJq\nxmAhSZJqxmAhSZJqxmAhSZJqxmAhSZJqxmAhSZJqpqpgERGfjIg7ImJ5RCyJiKsjYodB1nlvRHRE\nRHvpuSMiXhxe2ZIkqRFVO2JxAHAhsDfwemAicGNETBlkvWXAjLLH1lVuV5IkNYEJ1TROKR1RPh0R\nxwNPA7OA3w+8anqm6uokSVJTGe4xFhsBCXh+kHZTI+LRiHgsIn4eETsPc7uSJKkBxVBvmx4RAfwC\n2DCldOAA7fYBXgncA0wH/h14LbBLSumJftZ5KXA48CiwakgFSpI0Nk0GtgFuSCk9V++NDydYfJP8\ny3+/lNLiKtabANwHXJlS+s9+2rwLuGJIhUmSJIBjU0pX1nujVR1j0Skivg4cARxQTagASCmti4gF\n5FGM/jwKcPnll7PTTjsNpUQNwdy5czn33HOLLmNMsc/rzz6vP/u8vu677z7e/e53Q+l3ab1VHSxK\noeLNwIEppceGsP444NXAdQM0WwWw0047MXPmzGo3oSGaPn26/V1n9nn92ef1Z58XppBDCaoKFhFx\nEdACHAn8LSI2Ly1allJaVWpzCfBESun00vQZwHzg/8gHe36cfLrp92ryCSRJUsOodsTiQ+SzQG7u\nMf+fgUtLr7cC2suWvQT4Dvn6FUuBVmB2Sun+aouVJEmNrdrrWAx6empK6eAe06cAp1RZlyRJakLe\nK0R/19LSUnQJY459Xn/2ef3Z52PLkE83HUkRMRNobW1t9YAfSZKq0NbWxqxZswBmpZTa6r19Rywk\nSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLN\nGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwk\nSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNGCwkSVLNNHSwWL68\n6AokSVI1GjpYLFtWdAWSJKkaDR0s1q0rugJJklSNhg4Wq1YVXYEkSapGQweLF14ougJJklSNhg4W\nX/lK0RVIkqRqNHSweOqpoiuQJEnVaOhgcfzxRVcgSZKq0dDBoqOj6AokSVI1GjpYrFxZdAWSJKka\nBgtJklQzDR0sbr216AokSVI1GjpYLF5cdAWSJKkaDR0sJElSc2noYLHllkVXIEmSqtHQwWLt2qIr\nkCRJ1WjoYLF6ddEVSJKkajR0sFizpugKJElSNQwWkiSpZho6WKxbB0uWFF2FJEmqVEMHC4CFC4uu\nQJIkVaqqYBERn4yIOyJieUQsiYirI2KHCtY7OiLui4iVEXF3RLyh0m1OmVJNhZIkqUjVjlgcAFwI\n7A28HpgI3BgR/f76j4jZwJXAd4HdgZ8DP4+InSvZ4PjxVVYoSZIKM6GaximlI8qnI+J44GlgFvD7\nflb7CPDfKaWvlab/MyIOA04GThpsm96ITJKk5jHcYyw2AhLw/ABtZgM39Zh3Q2n+oF58cWiFSZKk\n+htysIiIAM4Dfp9SuneApjOAnud2LCnNH5QjFpIkNY+qdoX0cBGwM7DfENYN8kjHIObyhS9M5+KL\nu+a0tLTQ0tIyhE1KkjS6zJs3j3nz5nWbt2zZsoKqyYYULCLi68ARwAEppcFubv4UsHmPeZvRexSj\nj+2cy4knzuTEE4dSpSRJo1tff2y3tbUxa9asgioawq6QUqh4M/C6lNJjFaxyG3BIj3mHluYPaP31\nYcWKaiuUJElFqWrEIiIuAlqAI4G/RUTnSMSylNKqUptLgCdSSqeXlp0P3BIRpwC/LK0/C/jgYNub\nMsVgIUlSM6l2xOJDwDTgZuDJsscxZW22ouzAzJTSbeQwcQJwF/BW4M2DHPAJOGIhSVKzqfY6FoMG\nkZTSwX3M+3/A/6tmW5BHLP72t2rXkiRJRWnoe4Wstx6sXl10FZIkqVINHyxWrSq6CkmSVCmDhSRJ\nqpmGDxbuCpEkqXk0fLBwxEKSpObR0MFi0iRHLCRJaiYNHSwcsZAkqbk0dLB48UW4886iq5AkSZVq\n6GBx/fVFVyBJkqrR0MGiU6rgBuuSJKl4DR0s3vWu/LxmTbF1SJKkyjR0sNh11/y8cmWxdUiSpMo0\ndLCYNCk/GywkSWoODR0sJk/OzwYLSZKaQ0MHC0csJElqLgYLSZJUMwYLSZJUMw0dLNZfPz8//3yx\ndUiSpMo0dLCYNi0/r1hRbB2SJKkyDR0s3BUiSVJzaehgMa5U3V13FVuHJEmqTEMHi05XXFF0BZIk\nqRITii5gMNtsAy0tRVchSZIq0fAjFhMmQHt70VVIkqRKNHywGD/eYCFJUrNo+F0hixbB1lsXXYUk\nSapEw49YANx4Y9EVSJKkSjRFsJAkSc3BYCFJkmrGYCFJkmrGYCFJkmqm4YPFscd23TNEkiQ1toYP\nFttvDy99adFVSJKkSjR8sJg82bubSpLULBo+WEyZAqtWFV2FJEmqRMMHi84Ri5SKrkSSJA2m4YPF\nlCn5ec2aYuuQJEmDa/hgMXlyfvY4C0mSGl/DB4vOEQuPs5AkqfE1fLBwxEKSpObR8MHCEQtJkppH\nwwcLRywkSWoeDR8sHLGQJKl5NHyw6ByxMFhIktT4Gj5YdI5YuCtEkqTG1/DBwhELSZKaR8MHC0cs\nJElqHg0fLCZNys+OWEiS1PgaPlhE5Od77im2DkmSNLiGDxadLryw6AokSdJgmiZYbLxx0RVIkqTB\nVB0sIuKAiLgmIp6IiI6IOHKQ9geW2pU/2iNis0q3ue++cOSAW5EkSY1gKCMWGwB3AR8GUoXrJGB7\nYEbp8bKU0tOVbnDSJFi9utoyJUlSvU2odoWU0vXA9QARnYdWVuSZlNLyarcHsN56BgtJkppBvY6x\nCOCuiHgyIm6MiH2rWXnSJFizZoQqkyRJNVOPYLEYOBF4G/BW4C/AzRGxe6Vv4K4QSZKaQ9W7QqqV\nUnoAeKBs1vyI+AdgLvDegdadO3cu06dPp60tX3nzyCOhpaWFlpaWkSxZkqSmMG/ePObNm9dt3rJl\nywqqJouUKj3+so+VIzqAt6SUrqlyvbOA/VJK+/WzfCbQ2traysyZM/nAB+BPf4L584dcqiRJY0Jb\nWxuzZs0CmJVSaqv39ou6jsXu5F0kFVm6FG6/fQSrkSRJNVH1rpCI2AB4JfmATIDtImI34PmU0l8i\n4kxgi5TSe0vtPwI8AiwEJgMfBF4HHFrpNq+6qtoqJUlSEYZyjMWewG/J16ZIwDml+ZcA7yNfp2Kr\nsvbrldpsAbwI3AMcklL6nyHWLEmSGtRQrmNxCwPsQkkp/XOP6bOBs6svTZIkNZumuFfI+uvn5zvu\nKLYOSZI0sKYIFpuV7iry1FPF1iFJkgbWFMFi8uT8vGpVsXVIkqSBNUWwmDIlPxssJElqbE0RLN7x\njvy88cbF1iFJkgbWFMHibW/Lz1OnFluHJEkaWFMEi/XWy8/uCpEkqbE1RbBob8/P73tfsXVIkqSB\nNUWw6Dy2YnHFdxeRJElFaIpgMX160RVIkqRKNEWwkCRJzWEoNyErxFZbwY47Fl2FJEkaSNOMWOy7\nL6RUdBWSJGkgTRMspk2D5cuLrkKSJA2kaYLFhhsaLCRJanRNEyymTYMXXii6CkmSNJCmChaOWEiS\n1NiaKlisWNF1FU5JktR4miZYbLhhfl6xotg6JElS/5omWEyblp89zkKSpMbVdMHC4ywkSWpcTRMs\npk7Nz1//erF1SJKk/jVNsJg0KT9/85vF1iFJkvrXNMFi++3zs/cLkSSpcTVNsBhXqnTRomLrkCRJ\n/Wuau5sC7LQTdHQUXYUkSepP04xYAOy1F2yySdFVSJKk/jRVsGhvh1tvLboKSZLUn6YKFldckZ9T\nKrYOSZLUt6YKFp3WrSu6AkmS1JemChb77Zeff/3rYuuQJEl9a6pg0Xln04ULi61DkiT1ramCxcc+\nlp9PPbXYOiRJUt+aKljsuWfRFUiSpIE0VbDovF+IJElqTE0VLCZPLroCSZI0kKYKFhMnFl2BJEka\nSFMFi6lTu14/91xxdUiSpL41VbAod+ONRVcgSZJ6atpgsXp10RVIkqSemjZYLFhQdAWSJKmnpgsW\nxx6bny+4oNg6JElSb00XLD7ykfwcUWwdkiSpt6YLFp2BwlunS5LUeJo2WEiSpMZjsJAkSTXTdMFi\n8827Xj//fHF1SJKk3pouWLz85V2vf/zj4uqQJEm9NV2wKPfXvxZdgSRJKtfUwcLbqEuS1FiaOlh8\n7GNFVyBJkspVHSwi4oCIuCYinoiIjog4soJ1DoqI1ohYFREPRMR7h1ZudtVVw1lbkiSNlKGMWGwA\n3AV8GBj0MlURsQ1wLfBrYDfgfOB7EXHoELYNwN57D3VNSZI0kiZUu0JK6XrgeoCIiq4q8S/Awyml\nj5emF0XE/sBc4FfVbh9giy3K6/HaFpIkNYp6HGOxD3BTj3k3ALNr8ebLltXiXSRJUi3UI1jMAJb0\nmLcEmBYRwz6v453vHO47SJKkWql6V0iNdO68GPAYjblz5zJ9+vRu81paWmhpafn79A031Lw2SZKa\nwrx585g3b163ecsKHsqvR7B4Cti8x7zNgOUppTUDrXjuuecyc+bMEStMkqRm1vOPbYC2tjZmzZpV\nUEX12RVyG3BIj3mHleZLkqRRZCjXsdggInaLiN1Ls7YrTW9VWn5mRFxStsq3gH+IiK9ExI4RcRLw\nduBrwyn8rruGs7YkSRoJQxmx2BNYALSSj5E4B2gDPltaPgPYqrNxSulR4I3A68nXv5gLvD+l1PNM\nkarstttw1pYkSSNhKNexuIUBAklK6Z/7Wae4HT6SJKkumvpeIZ0+/OGiK5AkSTBKgsVFFxVdgSRJ\ngiYPFhMnFl2BJEkq19TBwtumS5LUWJo6WJx2WtfrxYuLq0OSJGVNHSw22qjr9VVXFVeHJEnKmjpY\nlDv55KIrkCRJTR8sPvGJrte3eZFwSZIK1fTB4pRTul7vu29xdUiSpFEQLDbdtPv0PfcUU4ckSRoF\nwaKnyy4rugJJksauURcsfvzjoiuQJGnsGnXB4i9/KboCSZLGrlEXLCRJUnFGRbBYsKD79C23FFOH\nJElj3agIFrvv3n36i18spg5Jksa6UREsevrVr4quQJKksWnUBItrrim6AkmSNGqCxZve1H3as0Mk\nSaq/URMsIrpPP/FEMXVIkjSWjZpg0dOiRUVXIEnS2DOqgsVLX9r1+vjjYd26wkqRJGlMGlXB4uij\nu09/6lPF1CFJ0lg1qoJFzyBx1lmwalUxtUiSNBaNqmCx5Za95+2/f/3rkCRprBpVwaIvra3Q3l50\nFZIkjQ2jLljccUfveRMm1L8OSZLGolEXLPbaq+gKJEkau0ZdsOjPH/5QdAWSJI1+ozJY/O53veft\nt1/965AkaawZlcGi523UO519NnR01LcWSZLGklEZLKZOhfXX7z3/4x+HG26ofz2SJI0VozJYABxz\nTN/zPfVUkqSRM2qDxXe/W3QFkiSNPaM2WPR37QqPsZAkaeSM2mABfV/i+/Ofr38dkiSNFaM6WDz8\nMNx2W/d5d94Jd99dTD2SJI12ozpYTJwI++zTe763U5ckaWSM6mDRn1/+Eq69tugqJEkafcZksACP\ntZAkaSSMiWBx1FG9591xB6xdW/9aJEkazcZEsLjqKvjMZ3rP/+pX616KJEmj2pgIFgBnnAHHHdd9\n3umnw5IlxdQjSdJoNGaCxbhxcNBBvec//njdS5EkadQaM8EC4N3v7j1vzz3rX4ckSaPVmAoW661X\ndAWSJI1uYypYAOy4Y+95e+0FDzxQ/1okSRptxlywmDSp97w77+w7cEiSpOqMuWBx9dVw/vl9L/vN\nb+pbiyRJo82YCxbbbQf/9m+w7ba9lx1ySP3rkSRpNBlzwaJTf8dUPPhgfeuQJGk0GVKwiIgPR8Qj\nEbEyIuZHxF4DtH1vRHRERHvpuSMiXhx6ybUxYULf83fYAW6+GRYvrms5kiSNClUHi4h4B3AO8J/A\nHsDdwA0RsckAqy0DZpQ9tq6+1Nq78ca+57/udbDFFvWtRZKk0WAoIxZzgW+nlC5NKd0PfAh4EXjf\nAOuklNIzKaWnS49nhlJsrR16KLS25lEKSZI0fFUFi4iYCMwCft05L6WUgJuA2QOsOjUiHo2IxyLi\n5xGx85CqHQEzZ8Ldd/e9bOut4dvfrm89kiQ1s2pHLDYBxgM9b921hLyLoy+LyKMZRwLHlrb5h4h4\neZXbHjGTJ0NKvec/9hh8/OP1r0eSpGbVzyGMVQugj1/NkFKaD8z/e8OI24D7gBPIx2n0a+7cuUyf\nPr3bvJaWFlpaWoZbb58+/Wn4whe6z1u+HC69FN7znhHZpCRJQzZv3jzmzZvXbd6yZcsKqiaL1Nef\n6v01zrtCXgTellK6pmz+xcD0lNJRFb7PT4C1KaVj+1k+E2htbW1l5syZFdc3XIsX93/QZhXdJElS\nYdra2pg1axbArJRSW723X9WukJTSWqAV+PulpCIiStN/qOQ9ImIc8Gqg4U7oHDdAb9x/P7S3168W\nSZKa0VDOCvkacEJEvCciXgV8C1gfuBggIi6NiC91No6IMyLi0IjYNiL2AK4gn276vWFXX2Mbbwy7\n7973sp12grlz61uPJEnNpupgkVL6CfAx4HPAAmBX4PCyU0i3pPuBnC8BvgPcC/wSmArMLp2q2lAm\nToQFC2CrrfpefuGF9a1HkqRmM6SDN1NKFwEX9bPs4B7TpwCnDGU7RbnySjjggKKrkCSp+YzZe4UM\nZP/94SMf6XvZ0qX1rUWSpGZisOjHeef1PX/jjfNBnE8/Df/1X/WtSZKkRler61iMKRMmwC67wMKF\ncN998KpXFV2RJEmNwRGLAVx5Zf/LFi7Mz69+dX1qkSSpGRgsBtDSAr/61cBtvLaFJEldDBaDeP3r\n88jFZpv132bRovrVI0lSIzNYVKClBZb0vO1aGY+xkCQpM1jUyJ/+VHQFkiQVz2BRhW9+E3bYoe9l\nr3kNXHttfeuRJKnRGCyq8KEPDTwyMWcOfP/73glVkjR2GSyqNHHiwMHhAx+AU5rqAuaSJNWOwWKI\nFi6Ez36272XlV+2cPz/f1MxLgUuSxgKDxRDtvDP8x3/0vzwCzjwTZs+Gxx+Hz32ufrVJklQUg8Uw\nXX55/8tOP73rdX/3HpEkaTQxWAzTscfCypWVtR3sKp6SJDU7g0UNTJ4My5fDjBkDtzvsMHj0UXjq\nKc8ckSSNTgaLGtlww3y66WC23RZe9jK44oqRr0mSpHozWNTQmWfCkUfmW6kP5rjjHLWQJI0+Bosa\neulL4b/+q/J7h4wbB//6r/DssyNblyRJ9WKwGCHt7bBgATz33MDtvv512HRT+OlP61OXJEkjyWAx\nQsaNg913h403rqz9iSeObD2SJNWDwaIOXngBnnhi4DZLl8K6ddDRUZ+aJEkaCQaLOpg6FbbYAt7/\n/oHbTZwI48fDH/9Yn7okSao1g0Udfe978I//mF//6Ef9t9t1VzjtNPjd7+pTlyRJtWKwqLOLL85n\nghxzzMDtzjoLXvtaWLSoLmVJklQTBos622knuOCCfJOylODFFwdu/6pXwXXXwS9+kdfx1FRJUiOb\nUHQBY92UKbBmDay3Xv9t3vjGrtePPw6bbDLydUmSNBSOWDSAiRPzcRWVuOACePrpfJ0MSZIajcGi\nQSxYAF/4AixbNvAlwX/4Q9h8c5gwIY90PP443Hpr/eqUJGkgBosGMW4cfOpTMG1aPq7ihz8cfJ1J\nk2CrrWD//eGuu+D55+Hyy0e+VkmS+mOwaFDHHw9/+1vl7ffYI9+r5Ljj4C9/qX57998Pl1xS/XqS\nJJUzWDSw9deHL38ZfvnL6tbbbru8S6Uau+ySw4x3XJUkDYfBosGddhoccQQ8+WRlt2OHfGnwjTaC\nq6+ufDudlxJ/4IHqa5QkqZPBokm87GX52It16+CLX6xsnbe+NR+H8b//m6dXr4YlS+Df/z2/7stv\nflObeiVJY5PBosmMHw+nn553WTz4YJ738pf3337NmnwZ8TvugK23hhkz4KtfhZNOyhfcevLJ7u1P\nOgnuvXfk6pckjW4Giyb2ylfmgPH44/neI8ce23/bvffOoxWdfvCD/Hzeeb2Px3jhhdrXKkkaGwwW\no8Q73pFPNa324Muzz87HY5TzAE5J0lAZLEah1lb47W+Hvv7s2flYi8cfz9OV3NNEkiQwWIxKM2fC\nQQflQHDQQXnevvtW9x6HHJIvvnX77fniXRtsAI88ks8aeeaZWlcsSRotvAnZKFc+cpESfOMbcO21\ncMMNla2/zz5dr7fbrvt7pQSzZsFRR8H228Ob3gRTp/b9PtdeCwccANOnV/8ZJEnNwxGLMSQCTj4Z\nrr++Kxj84Q9dy2+5pfL32nTTPJKxYAH8x39ASwuceGJe9uyzcP75cOON8Ne/5mtkzJkD739/bT+P\nJKnxOGIxxs2eDUuXwsqV+VoZDz8M735398DRl2ef7T3vyivztTPe/va+13n00WGXK0lqcI5YiI02\nyqECYNtt891Sr756aHdN7S9UQD6oNCI/3vjGPK+jo7qRkuGIyNfwkCSNHIOF+vSWt+QDPjt3mfz5\nz7BwYe3e/7rr8o3Txo/PB5huuCFssknefXLllb1HRJ55prqbsvXnwguH/x6SpP4ZLFSRV7wCdt65\nK2ikBOeeO7z3vOuurtcrVsBzz8Hhh+cLfW26aR5h+NnP4KGHYLPN8t1bO3V05ANT9967spu0dR7E\nunLl8GqWJA3MYKEh++hHu0LG88/neRGw+ebwwQ/m3SrDdfTR+QqjkO9v0rkrZfx4OPjgfKnyN70p\nL1+wIAeUu+7quqlap4MPzs/PPNN9NOTGG/OZLXffDW9+cz4ItdOf/tT7fSRJA/PgTdXES17S/xU7\nv/KVfIDoEUfATTfBD3/YdfGtWonoPa+znnXrus/fdFNYvDjfN+Xww/O83Xfv3uZzn4PXvAY+/Wn4\n/OcH3nbn+0/wf5MkEakBr98cETOB1tbWVmbOnFl0ORoBjz4K8+fDmWfCqafCj39c2S6Netl88657\nq9x9N+ywA0yenKcfeiiPdJx4Yr7J25QpeX5KcMYZeRSl0uuESFKttbW1MWvWLIBZKaW2em/fv7FU\niG22yY93vjNPH3dc17L//u98x9YddoAvfSmPcOy9d25/zjn1qa/8hm277db1+ogj8oGnkANFee79\nxCfy6AzArrvCH/8Iv/tdDhmXXQaHHQZ77gknnJBHcFavhlWr8ueCvNslAh57LB8s+9rX5gA2bVo+\nvuTOO/Num7VrYeLE/mt/7LF8TEzRlizJAU3SGJNSargHMBNIra2tSSrX0ZHST36S0je+kdK6dXl6\n7tzyQ0ob/7Heet2nly7NnwNS+uhHB173rW/Nz8cf33f/3HNPXv6zn3XNmz8/paefTunJJ/teZ+3a\nlDbeOKWbbqrdv9NDD+U6rrhi+O/1v/+b0vnnD/99pLGitbU1AQmYmYr4HV7ERgctymBRiCuvvLLo\nEmrmqadS+td/zT/h22+fn//yl5TWrEnpuuv6/qV95plFBI0rh/0eV1+d0ne+k9IJJ3SfP29eSief\n3H3eySfnz79kSUo33pj7pHz5ww+n9NOfpvS1r/Xdr7/6VUrLlqXU3p7So4/2Xn7ppSm9/e0pHXJI\n13tWor09pT32SOnWW3svK3+f/fdP6fTTc7h8xStS6u8rYu3alJ55pvf8pUuL/Tl/5pnuoW+sGE3f\nLc2gKYMF8GHgEWAlMB/Ya5D2RwP3ldrfDbxhkPYGiwLMmTOn6BIawm9/m9Ijj6R0wQX1CBZzCggz\nlT/+7//yL8NK2nYGuJ6PV72q6/XZZ+fnBx5I6Uc/yq/XrEnp5pu72jz4YNe/xXPPDb7d667rGvXp\n1Feo6Zy3336V/5x/7GN5ncsu63v56tUpzZ6d0sKFXfMuvTSltra+2++xR36/++6ruISKPfFEDmbP\nPZen16zpf5SqGtdfn9InPpHfL6WUnn02paOOSulTn+qaN5COjpR23nlOWrFi+LWoMk0XLIB3AKuA\n9wCvAr4NPA9s0k/72cBa4BRgR+CzwGpg5wG2YbAogMGiOg89lH9BPvdcDiFve1tKX/5y/mv6kktS\nuv/+vAti+vT8P23q1JRmzOj5i7Gxg0VRj5NPTmm//apf79hjUzr44N7zL7qoe5+vWJHSnDl5VOva\na/O/Z+fyb30rpcWLe7/HG96Q0iab5NGQlFJ64YXuy488MqVrrumaTin/bCxenNKqVSl96Uvd2y9a\nlHc//fWvue3KlV21br99Sueck+d3dOSRpXJr1uRdgZtvntKpp3avH/LP5vvel1+3t3dft6Mj/3x+\n+9u9f6bb2/Pn6rRuXff33XLL7tPf/35+v4ECxs9+lvt8l10G/z/Vs5Z166pbJ6Xu/d/phRfyv99T\nT/W9zuLFuc96Ou64vHuyWu3tOXT2ZfXq7iG4L0uW5Jo6rVuXQ36nFSvyz0t/mjFYzAfOL5sO4HHg\n4/20/xFwTY95twEXDbANg0UBDBb1N2fOnPTnP+e/eO+9N6Vbbsm/RG6/vftQ/pIl+X/rt7+d0sUX\np3TGGSltsEFKr371yP6CH52P5g5ze+9d/TqrVuWfq3326Xt5W1sOyZ3T06en9MY35l+slW7joYdy\n6Fq1KgeXgfr8rLNSuvzyrun118+71h55JKVzz+2+7gUX5PC3dGnX/4cHH0zp8cdTOvDAvKxzNGTB\ngq71HnuxAA4rAAAJgklEQVQsz2tvz7sKO+fPmdM1ivbZz6b0xz92397RR+fnI47omnfeed3/365c\nmdK//EtXKOz0+993X+8zn8mPgw/OdTz5ZJ5/2mld68yfnz975zqdx1tB/j9evut2m226Xu+4Y9d7\ndHR0hZWOjpRuvrmJggUwsTT6cGSP+RcDV/ezzp+Bf+sx7zPAggG2Y7AogMGi/urR5+VfOCtWpHTV\nVflL+oQT8qjKokX5i3LFivwX6Fe/mg+W7OuXx5Zb5sdJJ+UDaAf6RXPqqZX/Uqrvo7mDRXM+Rk+f\nH3NM9+nNNut9fFPxj2KDRbWnm24CjAeW9Ji/pLSboy8z+mk/Y4DtTAa47777qixPw7Fs2TLa2up+\nyvOYVkSfb711vovtiSd23X/l3nvzc/mFwlpbB3+vwdqUX8kU8lfe2rVd1/8YP773Oh0dsGxZPuX2\nJS/Jl3MfNy63ffDBfHruypV52fLl+bTdn/40X/DsnHOgvR1+8APYfvt8qm9ra75c/GtfC5MmwTe+\nsYylS9tobx/88517Ltx2G/zkJ4O31UCWAaPju6Xnz8LTT8N3vlNMLf37++/OyUVsvaoLZEXEy4An\ngNkppdvL5p8F7J9S2rePdVYD70kp/bhs3knAp1NKW/SznXcBV1RcmCRJ6unYlNKV9d5otSMWzwLt\nQM/L3mxG71GJTk9V2R7gBuBY4FHygaKSJKkyk4FtyL9L667qS3pHxHzg9pTSR0rTATwGXJBSOruP\n9j8CpqSU3lw271bg7pTSScMpXpIkNZahXNL7a8AlEdEK3AHMBdYnH8BJRFwKPJ5SOr3U/nzglog4\nBfgl0ALMAj44vNIlSVKjqTpYpJR+EhGbAJ8j7+K4Czg8pfRMqcmWwLqy9rdFRAvwxdLjQeDNKaV7\nh1u8JElqLA15d1NJktScxhVdgCRJGj0MFpIkqWYaLlhExIcj4pGIWBkR8yNir6JragYR8cmIuCMi\nlkfEkoi4OiJ26NFmUkR8IyKejYgXIuJnEbFZjzZbRcQvI+JvEfFURJwVEeN6tDkoIlojYlVEPBAR\n763HZ2x0pX+Djoj4Wtk8+7zGImKLiLis1KcvRsTdETGzR5vPRcSTpeW/iohX9lj+koi4IiKWRcTS\niPheRGzQo82uEfE/pe+iP0fEv9fj8zWaiBgXEZ+PiIdL/fl/EfHpPtrZ50MUEQdExDUR8UTpO+TI\nPtrUpX8j4uiIuK/U5u6IeEPVH6iIy33296DKG5z56NZ31wHHATsBrwGuJV8HZEpZm2+W5h0I7AH8\nAfhd2fJxwB/J5z6/BjgceBr4QlmbbYAVwFnkq61+mHyZ90OL7oOC+38v4GFgAfA1+3zE+nkj8p2V\nv0c+u2xr4PXAtmVtTit9b8wBXg38HHgIWK+szX+TLwW5J7Av8ABwednyDYHFwCWl/1PHAH8DPlB0\nHxTQ56eXfib/CXgF8FZgOXCyfV6zPv4n8gkRbyFfK6rnbTPq0r8M4aahfX6eoju0R+dVdYMzHwP2\n5SZAB/mKqADTSj8gR5W12bHU5h9L028o/VBtUtbmRGApMKE0/RXgnh7bmgdcV/RnLrCvpwKLgIOB\n31IKFvb5iPT1l4FbBmnzJDC3bHoasBI4pjS9U+nfYI+yNoeTz2abUZr+F/IFASeUtTkTuLfoPiig\nz38BfLfHvJ8Bl9rnI9LfHfQOFnXpX4Zw09C+Hg2zKyQiJpL/Avl157yUP9VN5BSl6mxEvgnN86Xp\nWeTTi8v7dxH54mad/bsP8MeU0rNl73MDMB3YpazNTT22dQNj+9/oG8AvUkq/6TF/T+zzWpsD3BkR\nPynt8muLiA90LoyIbcn3ISrv8+XA7XTv86UppQVl73sT+f/L3mVt/ieltK6szQ3AjhExvdYfqsH9\nATgkIrYHiIjdgP3Io6T2+Qirc//OpgbfNQ0TLBj4BmcD3bBMPUREAOcBv09d1wuZAawp/UCWK+/f\n/m4YRwVtpkXEpOHW3mwi4p3A7sAn+1i8OfZ5rW1H/strEXAY8C3ggoh4d2n5DPKX6UDfIzPIQ/t/\nl1JqJ4fwav5dxoovAz8G7o+INUArcF5K6Uel5fb5yKpn/w7lpqG9DOXKm/UW5E5V5S4Cdgb2r6Bt\npf07UJuooM2oExFbkgPcoSmltdWsin0+VOOAO1JKZ5Sm746IXchh4/IB1qukzwdrM1b7/B3Au4B3\nAveSg/T5EfFkSumyAdazz0dWrfq3kjZV9X8jjVgM5QZn6iEivg4cARyUUnqybNFTwHoRMa3HKuX9\n29cN4zYvW9Zfm82A5SmlNcOpvQnNAjYFWiNibUSsJR+k+ZHSX3ZLgEn2eU0tpuye0CX3kQ8qhNxX\nwcDfI0+Vpv8uIsYDL2HwPoex9310FnBmSumnKaWFKaUrgHPpGqWzz0fWSPdv+WjIUG4a2kvDBIvS\nX3ytwCGd80pD+oeQ9/FpEKVQ8WbgdSmlx3osbiUfyFPevzuQv5A7+/c24DWRL9ne6TBgGV1f5reV\nv0dZm9tq8RmazE3kMzl2B3YrPe4k/+Xc+Xot9nkt3Uo+ALbcjsCfAVJKj5C/HMv7fBp5P3N5n28U\nEXuUvcch5C/vO8ravLb05dzpMGBRSmlZbT5K01if3n+xdlD6/WGfj6w6929f3zWHUu13TdFHwPY4\n+vQY8pGu5aebPgdsWnRtjf4g7/5YChxATpydj8k92jwCHET+a/tWep/6eDf5tKVdyUcVLwE+X9Zm\nG/Kpj18hf6GfBKwBXl90HzTCg7KzQuzzEenfPcln2nwS+AfyEP0LwDvL2ny89L0xhxz8fk6+R1H5\nqXnXkYPfXuQDERcBl5Utn0Y+Ev8S8m7Fd5T+Dd5fdB8U0Oc/JB9wfAT59N6jyPvzv2Sf16yPNyD/\nMbI7ObR9tDS9VT37l3yQ5hq6Tjf9DPkSEM17umnpg51EPu9/JTkl7Vl0Tc3wKP0wtvfxeE9Zm0nA\nheTdTi8APwU26/E+W5GvgbGC/AvuK8C4Hm0OJI+ArCz9cB9X9OdvlAfwG7oHC/u89n18BHAP8CKw\nEHhfH20+U/oSfZF8VPsreyzfiDyytIwcyL8LrN+jzWuAW0rv8RhwatGfvaD+3oB8V+tHyNc9eJB8\nfYMJPdrZ50Pv4wP7+Q7/Qb37F3gbcH/pu+Ye8k1Gq/o83oRMkiTVTMMcYyFJkpqfwUKSJNWMwUKS\nJNWMwUKSJNWMwUKSJNWMwUKSJNWMwUKSJNWMwUKSJNWMwUKSJNWMwUKSJNWMwUKSJNXM/wdjWDGY\nYxHdnwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d2044e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.variable_scope(\"Decoder\") as scope:\n",
    "#     attention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "    \n",
    "#     (attention_keys,\n",
    "#      attention_values,\n",
    "#      attention_score_fn,\n",
    "#      attention_construct_fn) = (\n",
    "#         seq2seq.prepare_attention(\n",
    "#             attention_states,\n",
    "#             attention_option=\"luong\", \n",
    "#             num_units=decoder_hidden_units)\n",
    "#     )\n",
    "    \n",
    "#     decoder_fn_train = seq2seq.attention_decoder_fn_train(\n",
    "#               encoder_state=encoder_state,\n",
    "#               attention_keys=attention_keys,\n",
    "#               attention_values=attention_values,\n",
    "#               attention_score_fn=attention_score_fn,\n",
    "#               attention_construct_fn=attention_construct_fn)\n",
    "    \n",
    "#     decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "#     (decoder_outputs_train, decoder_state_train, _) = (\n",
    "#           seq2seq.dynamic_rnn_decoder(\n",
    "#               cell=decoder_cell,\n",
    "#               decoder_fn=decoder_fn_train,\n",
    "#               inputs=decoder_inputs,\n",
    "#               sequence_length=decoder_length,\n",
    "#               time_major=True,\n",
    "#               scope=scope))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
